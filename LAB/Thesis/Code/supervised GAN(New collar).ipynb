{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os, sys\n",
    "from os.path import join, dirname\n",
    "\n",
    "import datetime, time\n",
    "import csv\n",
    "from glob import glob\n",
    "import chardet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MaxAbsScaler, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, SimpleRNN, LSTM, GRU, Reshape, RepeatVector, Conv2DTranspose, Activation, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, MaxPooling2D, Bidirectional, TimeDistributed,  Attention, BatchNormalization, Dropout, Lambda, Conv1D\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import imblearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7                           # {\"0\" : \"Playing\", \"1\" : \"Talking\", \"2\" : \"Petting\", \"3\" : \"TV / Radio\", \"4\" : \"Eating / Cooking\", \"5\" : \"Moved It\", \"6\" : \"None of the above\", \"7\" : \"Other\"}\n",
    "time_offset = 5                          # 5초 단위 window: 50, 10초 단위 window: 90, 15초 단위 window: 128\n",
    "window_size = 50\n",
    "overlap_ratio = 0.5\n",
    "bi_class = 1                             # Binary Classification (1 : Playing or not, 2 : Talking or not, 3 : Petting or not, 4: TV / Radio or not, 5 : Eating / Cooking or not, 6 : Moved It or not)\n",
    "cross_val = 0\n",
    "rand_st=2\n",
    "mode = 0                                 # Split data {0: Didn't split, 1: US only, 2: Korea only, 3: train with US and test with Korea 4: train with Korea and test with US}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fname = '../Data/Preprocessed(new)/preprocessed_data(New collar_2).csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_fname)\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iaq = data['iaq']\n",
    "iaq_cat = []\n",
    "\n",
    "for num in iaq:\n",
    "    if num < 50 and num >=0:\n",
    "        iaq_cat.append('Good')\n",
    "    elif num >= 50 and num < 100:\n",
    "        iaq_cat.append('Average')\n",
    "    elif num >= 100 and num < 150:\n",
    "        iaq_cat.append('Little bad')\n",
    "    elif num >= 150 and num < 200:\n",
    "        iaq_cat.append('Bad')\n",
    "    elif num >= 200 and num < 300:\n",
    "        iaq_cat.append('Worse')\n",
    "    elif num >= 300 and num <= 500:\n",
    "        iaq_cat.append('Very bad')\n",
    "    else:\n",
    "        print(num)\n",
    "data['iaq_cat'] = iaq_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data,pd.get_dummies(data['sound category'])],axis=1)         # Onehot encode sound category\n",
    "data = pd.concat([data,pd.get_dummies(data['orientation_cat'])],axis=1)        # Onehot encode orientation category\n",
    "data = pd.concat([data,pd.get_dummies(data['iaq_cat'])],axis=1)                # Onehot encode iaq category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rowID list\n",
    "rowID_list = np.array(data['RowID'].drop_duplicates())\n",
    "data = data.to_records(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# data['pressure'] = scaler.fit_transform(data['pressure'].reshape(-1,1)).reshape(-1)\n",
    "data['gasResistance'] = scaler.fit_transform(data['gasResistance'].reshape(-1,1)).reshape(-1)\n",
    "data['staticIaq'] = scaler.fit_transform(data['staticIaq'].reshape(-1,1)).reshape(-1)\n",
    "data['co2Equivalent'] = scaler.fit_transform(data['co2Equivalent'].reshape(-1,1)).reshape(-1)\n",
    "data['breathVocEquivalent'] = scaler.fit_transform(data['breathVocEquivalent'].reshape(-1,1)).reshape(-1)\n",
    "data['audioLevel'] = scaler.fit_transform(data['audioLevel'].reshape(-1,1)).reshape(-1)\n",
    "data['rawTemp'] = scaler.fit_transform(data['rawTemp'].reshape(-1,1)).reshape(-1)\n",
    "data['rawHumidity'] = scaler.fit_transform(data['rawHumidity'].reshape(-1,1)).reshape(-1)\n",
    "data['pressure'] = scaler.fit_transform(data['pressure'].reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split US and Korea\n",
    "us_rowIDs = []\n",
    "korea_rowIDs = []\n",
    "\n",
    "if mode != 0:\n",
    "    for rowid in rowID_list:\n",
    "    #     print(rowid, rowid[0])\n",
    "        if rowid[0] == '1':\n",
    "            korea_rowIDs.append(rowid)\n",
    "        else:\n",
    "            us_rowIDs.append(rowid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_col_name = ['accX', 'accY', 'accZ', 'chord', 'orientation', 'ir', 'full', 'iaq', 'iaqAccuracy', 'rawTemp',\n",
    "#                     'pressure', 'rawHumidity', 'gasResistance', 'compGasAccuracy', 'gasPercentageAccuracy', 'temperature', \n",
    "#                     'humidity', 'staticIaq', 'statIaqAccuracy', 'co2Equivalent', 'co2Accuracy', 'breathVocEquivalent', \n",
    "#                     'breathVocAccuracy', 'audioLevel', 'Loud', 'Moderate', 'Quiet']\n",
    "feature_col_name = ['accX', 'accY', 'accZ', 'chord', 'full', 'iaq', 'rawTemp',\n",
    "                    'pressure', 'rawHumidity', 'gasResistance', 'staticIaq', 'co2Equivalent', 'breathVocEquivalent', \n",
    "                    'audioLevel', 'Loud', 'Moderate', 'Quiet', 'Landscape Left Back', 'Landscape Left Front', 'Landscape Right Back',\n",
    "                    'Landscape Right Front', 'Portrait Down Back', 'Portrait Down Front', 'Portrait Up Back', \n",
    "                    'Portrait Up Front', 'Average', 'Bad', 'Good', 'Little bad', 'Very bad', 'Worse']\n",
    "target_col_name = ['Modality_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num = len(feature_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "us_X = []\n",
    "korea_X = []\n",
    "\n",
    "Y = []\n",
    "us_Y = []\n",
    "korea_Y = []\n",
    "\n",
    "\n",
    "if mode != 0:\n",
    "    for rowID in us_rowIDs:\n",
    "        #Split raw data by rowID & split X, Y data\n",
    "        tmp_data = data[data['RowID'] == rowID]\n",
    "        feature = tmp_data[feature_col_name]\n",
    "        feature = np.array(feature.tolist())\n",
    "        target = tmp_data[target_col_name][0][0]\n",
    "        target = np.array(target.tolist())\n",
    "        us_X.append(feature)\n",
    "        us_Y.append(target)\n",
    "    \n",
    "    for rowID in korea_rowIDs:\n",
    "        #Split raw data by rowID & split X, Y data\n",
    "        tmp_data = data[data['RowID'] == rowID]\n",
    "        feature = tmp_data[feature_col_name]\n",
    "        feature = np.array(feature.tolist())\n",
    "        target = tmp_data[target_col_name][0][0]\n",
    "        target = np.array(target.tolist())\n",
    "        korea_X.append(feature)\n",
    "        korea_Y.append(target)\n",
    "\n",
    "else:\n",
    "    for rowID in rowID_list:\n",
    "        #Split raw data by rowID & split X, Y data\n",
    "        tmp_data = data[data['RowID'] == rowID]\n",
    "        feature = tmp_data[feature_col_name]\n",
    "        feature = np.array(feature.tolist())\n",
    "        target = tmp_data[target_col_name][0][0]\n",
    "        target = np.array(target.tolist())\n",
    "        X.append(feature)\n",
    "        Y.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bi_class != 0:\n",
    "    #Transit multi classification to binary classification\n",
    "    if mode != 0:\n",
    "        for idx in range(len(us_Y)):\n",
    "            if us_Y[idx] == bi_class-1:\n",
    "                us_Y[idx]=1\n",
    "            else:\n",
    "                us_Y[idx]=0\n",
    "                \n",
    "        for idx in range(len(korea_Y)):\n",
    "            if korea_Y[idx] == bi_class-1:\n",
    "                korea_Y[idx]=1\n",
    "            else:\n",
    "                korea_Y[idx]=0\n",
    "    else:\n",
    "        for idx in range(len(Y)):\n",
    "            if Y[idx] == bi_class-1:\n",
    "                Y[idx]=1\n",
    "            else:\n",
    "                Y[idx]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_preprocess(X, window_size, overlap_ratio):\n",
    "    #Transform data shape using the set time window\n",
    "    processed_X = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        tmp_X = X[i]\n",
    "        tmp = []\n",
    "        start_row = 0\n",
    "        end_row = start_row + window_size\n",
    "        \n",
    "        if len(tmp_X)%int(window_size*overlap_ratio) == 0:\n",
    "            for j in range(len(tmp_X)//int(window_size*overlap_ratio)-1):\n",
    "                tmp.append(tmp_X[int(start_row):int(end_row)])\n",
    "                start_row += (window_size*overlap_ratio)\n",
    "                end_row += (window_size*overlap_ratio)\n",
    "        else:\n",
    "            for j in range(len(tmp_X)//int(window_size*overlap_ratio)+1):\n",
    "                if end_row > len(tmp_X):\n",
    "                    \n",
    "                    tmp.append(tmp_X[-window_size:])\n",
    "                    start_row += (window_size*overlap_ratio)\n",
    "                    end_row += (window_size*overlap_ratio)\n",
    "                    break\n",
    "                else:\n",
    "                    \n",
    "                    tmp.append(tmp_X[int(start_row):int(end_row)])\n",
    "                    start_row += (window_size*overlap_ratio)\n",
    "                    end_row += (window_size*overlap_ratio)\n",
    "        processed_X.append(tmp)\n",
    "        \n",
    "    return processed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode != 0:\n",
    "    us_X = X_preprocess(us_X, window_size, overlap_ratio)        ### preprocess with input shape\n",
    "    korea_X = X_preprocess(korea_X, window_size, overlap_ratio)\n",
    "    if bi_class == 0:\n",
    "        ### onehot encode Y\n",
    "        us_Y = np.eye(num_classes)[us_Y]\n",
    "        korea_Y = np.eye(num_classes)[korea_Y]\n",
    "    else: \n",
    "        us_Y = np.eye(2)[us_Y]\n",
    "        korea_Y = np.eye(2)[korea_Y]\n",
    "\n",
    "else:    \n",
    "    X = X_preprocess(X, window_size, overlap_ratio)        ### preprocess with input shape\n",
    "    if bi_class == 0:\n",
    "        ### onehot encode Y\n",
    "        Y = np.eye(num_classes)[Y]\n",
    "    else: Y = np.eye(2)[Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample X Data size\n",
    "\n",
    "def subsample(X, min_us_len, min_korea_len):\n",
    "    sampled_X = []\n",
    "    addon = 0\n",
    "    \n",
    "    if min_korea_len > min_us_len:\n",
    "        if np.array(X).shape[1] == min_us_len:\n",
    "            return X\n",
    "        else:\n",
    "            interval = min_korea_len / min_us_len\n",
    "            quotient = int(np.modf(interval)[1])\n",
    "            remainder = np.modf(interval)[0]\n",
    "\n",
    "            for i in range(len(X)):\n",
    "                temp_X = []\n",
    "                for j in range(min_us_len):\n",
    "                    if addon >= 1:\n",
    "                        temp_X.append(X[i][j*quotient + 1])\n",
    "                        addon = 0\n",
    "                        addon += remainder\n",
    "                    else:\n",
    "                        temp_X.append(X[i][j*quotient])\n",
    "                        addon += remainder\n",
    "\n",
    "                sampled_X.append(temp_X)\n",
    "            \n",
    "    else:\n",
    "        if np.array(X).shape[1] == min_korea_len:\n",
    "            return X\n",
    "        else:\n",
    "            interval = min_us_len / min_korea_len\n",
    "            quotient = int(np.modf(interval)[1])\n",
    "            remainder = np.modf(interval)[0]\n",
    "\n",
    "            for i in range(len(X)):\n",
    "                temp_X = []\n",
    "                for j in range(min_korea_len):\n",
    "                    if addon >= 1:\n",
    "                        temp_X.append(X[i][j*quotient + 1])\n",
    "                        addon = 0\n",
    "                        addon += remainder\n",
    "                    else:\n",
    "                        temp_X.append(X[i][j*quotient])\n",
    "                        addon += remainder\n",
    "\n",
    "            sampled_X.append(temp_X)\n",
    "                    \n",
    "    return sampled_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to minimum length\n",
    "\n",
    "min_len = 99999999\n",
    "min_us_len = 99999999\n",
    "min_korea_len = 99999999\n",
    "min_X = []\n",
    "min_us_X = []\n",
    "min_korea_X = []\n",
    "\n",
    "if mode == 0:\n",
    "#     print('Start mode 0\\n')\n",
    "    for x in X:\n",
    "        if len(x) < min_len:\n",
    "            min_len = len(x)\n",
    "#     print(min_len)\n",
    "\n",
    "    for x in X:\n",
    "        min_X.append(x[:min_len])\n",
    "\n",
    "else:\n",
    "    for x in us_X:\n",
    "        if len(x) < min_us_len:\n",
    "            min_us_len = len(x)\n",
    "            \n",
    "    for x in korea_X:\n",
    "        if len(x) < min_korea_len:\n",
    "            min_korea_len = len(x)\n",
    "            \n",
    "    if mode == 1:\n",
    "        for x in us_X:\n",
    "            min_us_X.append(x[:min_us_len])\n",
    "        for x in korea_X:\n",
    "            min_korea_X.append(x[:min_korea_len])\n",
    "        \n",
    "    elif mode == 2:\n",
    "        min_korea_len = 60\n",
    "        for x in us_X:\n",
    "            min_us_X.append(x[:min_us_len])\n",
    "        for x in korea_X:\n",
    "            min_korea_X.append(x[:min_korea_len])\n",
    "        \n",
    "    else:\n",
    "        if min_korea_len < min_us_len:\n",
    "            min_len = min_korea_len\n",
    "        else: min_len = min_us_len\n",
    "\n",
    "        for x in us_X:\n",
    "            min_us_X.append(x[:min_len])\n",
    "\n",
    "        for x in korea_X:\n",
    "            min_korea_X.append(x[:min_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop duplicate\n",
    "\n",
    "if bi_class != 0:\n",
    "    \n",
    "    target_list = []\n",
    "    us_target_list = []\n",
    "    korea_target_list = []\n",
    "    del_list = []\n",
    "    us_del_list = []\n",
    "    korea_del_list = []\n",
    "    \n",
    "    if mode == 0:\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i][1] == 1:\n",
    "                target_list.append(i)\n",
    "\n",
    "        for i in target_list:\n",
    "            for j in range(len(min_X)):\n",
    "                if j in target_list:\n",
    "                    pass\n",
    "                else:\n",
    "                    if np.array_equal(np.array(min_X[i]), np.array(min_X[j])):\n",
    "                        if j not in del_list:\n",
    "                            del_list.append(j)\n",
    "        X = []\n",
    "        Target = []\n",
    "\n",
    "        for i in range(len(Y)):\n",
    "            if i not in del_list:\n",
    "                X.append(min_X[i])\n",
    "                Target.append(Y[i])\n",
    "                \n",
    "    else:\n",
    "        for i in range(len(us_Y)):\n",
    "            if us_Y[i][1] == 1:\n",
    "                us_target_list.append(i)\n",
    "\n",
    "        for i in us_target_list:\n",
    "            for j in range(len(min_us_X)):\n",
    "                if j in us_target_list:\n",
    "                    pass\n",
    "                else:\n",
    "                    if np.array_equal(np.array(min_us_X[i]), np.array(min_us_X[j])):\n",
    "                        if j not in us_del_list:\n",
    "                            us_del_list.append(j)\n",
    "                            \n",
    "        for i in range(len(korea_Y)):\n",
    "            if korea_Y[i][1] == 1:\n",
    "                korea_target_list.append(i)\n",
    "\n",
    "        for i in korea_target_list:\n",
    "            for j in range(len(min_korea_X)):\n",
    "                if j in korea_target_list:\n",
    "                    pass\n",
    "                else:\n",
    "                    if np.array_equal(np.array(min_korea_X[i]), np.array(min_korea_X[j])):\n",
    "                        if j not in korea_del_list:\n",
    "                            korea_del_list.append(j)\n",
    "        \n",
    "        us_X = []\n",
    "        us_Target = []\n",
    "        korea_X = []\n",
    "        korea_Target = []\n",
    "\n",
    "        for i in range(len(us_Y)):\n",
    "            if i not in us_del_list:\n",
    "                us_X.append(min_us_X[i])\n",
    "                us_Target.append(us_Y[i])\n",
    "                \n",
    "        for i in range(len(korea_Y)):\n",
    "            if i not in korea_del_list:\n",
    "                korea_X.append(min_korea_X[i])\n",
    "                korea_Target.append(korea_Y[i])\n",
    "\n",
    "else:\n",
    "    target_list = []\n",
    "    us_target_list = []\n",
    "    korea_target_list = []\n",
    "    del_list = []\n",
    "    us_del_list = []\n",
    "    korea_del_list = []\n",
    "    \n",
    "    if mode == 0:\n",
    "        X = min_X\n",
    "        Target = Y\n",
    "\n",
    "    else:\n",
    "        for i in range(len(us_Y)):\n",
    "            if us_Y[i][1] == 1:\n",
    "                us_target_list.append(i)\n",
    "\n",
    "        for i in us_target_list:\n",
    "            for j in range(len(min_us_X)):\n",
    "                if j in us_target_list:\n",
    "                    pass\n",
    "                else:\n",
    "                    if np.array_equal(np.array(min_us_X[i]), np.array(min_us_X[j])):\n",
    "                        if j not in us_del_list:\n",
    "                            us_del_list.append(j)\n",
    "\n",
    "        for i in range(len(korea_Y)):\n",
    "            if korea_Y[i][1] == 1:\n",
    "                korea_target_list.append(i)\n",
    "\n",
    "        for i in korea_target_list:\n",
    "            for j in range(len(min_korea_X)):\n",
    "                if j in korea_target_list:\n",
    "                    pass\n",
    "                else:\n",
    "                    if np.array_equal(np.array(min_korea_X[i]), np.array(min_korea_X[j])):\n",
    "                        if j not in korea_del_list:\n",
    "                            korea_del_list.append(j)\n",
    "\n",
    "        us_X = []\n",
    "        us_Target = []\n",
    "        korea_X = []\n",
    "        korea_Target = []\n",
    "\n",
    "        for i in range(len(us_Y)):\n",
    "            if i not in us_del_list:\n",
    "                us_X.append(min_us_X[i])\n",
    "                us_Target.append(us_Y[i])\n",
    "\n",
    "        for i in range(len(korea_Y)):\n",
    "            if i not in korea_del_list:\n",
    "                korea_X.append(min_korea_X[i])\n",
    "                korea_Target.append(korea_Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 0:\n",
    "#     X = subsample(X, min_us_len, min_korea_len)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Target, test_size=0.2)\n",
    "    \n",
    "elif mode == 1:\n",
    "    us_X = subsample(us_X, min_us_len, min_korea_len)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(us_X, us_Target, test_size=0.2)\n",
    "\n",
    "elif mode == 2:\n",
    "    korea_X = subsample(korea_X, min_us_len, min_korea_len)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(korea_X, korea_Target, test_size=0.2)\n",
    "\n",
    "elif mode == 3:\n",
    "    X_train = subsample(us_X, min_us_len, min_korea_len)\n",
    "    X_test = subsample(korea_X, min_us_len, min_korea_len)\n",
    "    Y_train = us_Target \n",
    "    Y_test = korea_Target\n",
    "\n",
    "else:\n",
    "    X_train = subsample(korea_X, min_us_len, min_korea_len)\n",
    "    X_test = subsample(us_X, min_us_len, min_korea_len)\n",
    "    Y_train = korea_Target\n",
    "    Y_test = us_Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape = np.array(X_train[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "### End Setup, separate model sections\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Replay #1 - GAN\n",
    "- https://keras.io/examples/generative/dcgan_overriding_train_step/\n",
    "- https://towardsdatascience.com/writing-your-first-generative-adversarial-network-with-keras-2d16fd8d4889\n",
    "- https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "- https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-1-dimensional-function-from-scratch-in-keras/\n",
    "- https://www.kaggle.com/function9/bidirectional-lstm-gan-music-generation\n",
    "- https://wiki.pathmind.com/generative-adversarial-network-gan\n",
    "- https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=chunjein&logNo=221589624838  \n",
    "    \n",
    "http://www.smartdesignlab.org/DL/%EC%8B%A0%EA%B8%B0%EC%88%A0/GAN_keras.html - 참고 필요  \n",
    "http://www.smartdesignlab.org/DL/GAN_tf2.html  \n",
    "https://deep-eye.tistory.com/63  \n",
    "https://velog.io/@hyebbly/Deep-Learning-Loss-%EC%A0%95%EB%A6%AC-1-GAN-loss\n",
    "\n",
    "https://machinelearningmastery.com/semi-supervised-generative-adversarial-network/  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Setup ###\n",
    "\n",
    "#Rebalance the data\n",
    "if bi_class == 0:\n",
    "    sm = imblearn.over_sampling.SMOTE()\n",
    "    X_shape = np.array(X_train).shape\n",
    "    Y_shape = np.array(Y_train).shape\n",
    "    new_X_train = np.array(X_train).reshape(X_shape[0], X_shape[1]*X_shape[2]*X_shape[3])\n",
    "    Y_train = np.array(Y_train).astype('float64')\n",
    "    X_train, Y_train = sm.fit_resample(new_X_train, Y_train)\n",
    "    temp = X_train.shape\n",
    "    X_train = X_train.reshape([temp[0], X_shape[1], X_shape[2], X_shape[3]])\n",
    "    Y_train = Y_train.reshape(temp[0], Y_shape[1])\n",
    "\n",
    "else:\n",
    "    sm = imblearn.over_sampling.SMOTE()         # random state do not set\n",
    "    origin_shape = np.array(X_train).shape\n",
    "    new_X_train = np.array(X_train).reshape(origin_shape[0], origin_shape[1]*origin_shape[2]*origin_shape[3])\n",
    "    Y_train = np.array(Y_train).astype('float64')\n",
    "    X_train, Y_train = sm.fit_resample(new_X_train, Y_train)\n",
    "    temp = X_train.shape\n",
    "    X_train = X_train.reshape([temp[0], origin_shape[1], origin_shape[2], origin_shape[3]])\n",
    "    Y_train = np.eye(2)[Y_train.reshape(temp[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom activation function\n",
    "def custom_activation(output):\n",
    "    logexpsum = K.sum(K.exp(output), axis=-1, keepdims=True)\n",
    "    result = logexpsum / (logexpsum + 1.0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # define the standalone supervised and unsupervised discriminator models\n",
    "def define_discriminator(in_shape=in_shape, RNN_unit=200, bi_class=bi_class, n_classes=num_classes):\n",
    "    # image input\n",
    "    in_image = Input(shape=in_shape)\n",
    "    # downsample\n",
    "    disc = Conv2D(31, 1, padding='same', name='disc_conv1')(in_image)\n",
    "    disc = BatchNormalization(momentum=0.8, name='disc_BM1')(disc)\n",
    "    disc = layers.LeakyReLU(alpha=0.2, name='disc_activ1')(disc)\n",
    "    disc = Dropout(0.2)(disc)\n",
    "    disc = Reshape((RNN_unit, -1), name='disc_reshape1')(disc)\n",
    "    disc = LSTM(units=RNN_unit, activation='tanh', recurrent_activation='sigmoid', return_sequences=False, name='disc_rnn1')(disc)\n",
    "#     disc = GRU(units=RNN_unit, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, name='disc_rnn2')(disc)\n",
    "    \n",
    "#     disc = Conv1D(128, 1, padding='same', name='disc_conv2')(disc)\n",
    "    \n",
    "#     disc = GlobalAveragePooling1D()(disc)\n",
    "\n",
    "    # flatten feature maps\n",
    "#     fe = Flatten()(disc)\n",
    "    # dropout\n",
    "#     fe = Dropout(0.4)(fe)\n",
    "    fe = Dense(32)(disc)\n",
    "    # output layer nodes\n",
    "    if bi_class == 0:\n",
    "        fe = Dense(n_classes)(fe)\n",
    "    \n",
    "        # supervised output\n",
    "        c_out_layer = Activation('softmax')(fe)\n",
    "        # define and compile supervised discriminator model\n",
    "        c_model = Model(in_image, c_out_layer)\n",
    "        c_model.summary()\n",
    "        c_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "#         c_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=[keras.metrics.CategoricalAccuracy(), keras.metrics.AUC(multi_label=True), tfa.metrics.F1Score(num_classes=num_classes)])\n",
    "    else:\n",
    "        fe = Dense(2)(fe)\n",
    "    \n",
    "        # supervised output\n",
    "        c_out_layer = Activation('softmax')(fe)\n",
    "        # define and compile supervised discriminator model\n",
    "        c_model = Model(in_image, c_out_layer)\n",
    "        c_model.summary()\n",
    "        c_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "#         c_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy', keras.metrics.AUC()])\n",
    "        \n",
    "    # unsupervised output\n",
    "    d_out_layer = Lambda(custom_activation)(fe)\n",
    "    # define and compile unsupervised discriminator model\n",
    "    d_model = Model(in_image, d_out_layer)\n",
    "    d_model.summary()\n",
    "    d_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "    \n",
    "    return d_model, c_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(input_shape, reshape=in_shape , feature_num=feature_num):\n",
    "    # image generator input\n",
    "    in_lat = Input(shape=(input_shape,))\n",
    "    # foundation for 7x7 image\n",
    "    gen = Dense(feature_num, use_bias=False, name='gen_Dense1')(in_lat)\n",
    "#     gen = BatchNormalization(momentum=0.8, name='gen_BM1')(gen)\n",
    "    gen = RepeatVector(276)(gen)\n",
    "    gen = layers.LeakyReLU(alpha=0.2, name='gen_activ1')(gen)\n",
    "    gen = LSTM(units=50, activation='tanh', recurrent_activation='sigmoid',return_sequences=True, name='gen_rnn1')(gen)\n",
    "#     gen = BatchNormalization(momentum=0.8, name='gen_BM2')(gen)\n",
    "    gen = tf.expand_dims(gen, -1)\n",
    "#     gen = Reshape((276,50,1), name='gen_reshape1')(gen)\n",
    "    gen_output = Conv2DTranspose(feature_num, 64, padding=\"same\", activation='relu', name='gen_conv1')(gen)\n",
    "    \n",
    "    model = Model(inputs=in_lat, outputs=gen_output, name='Generator')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect image output from generator as input to discriminator\n",
    "    gan_output = d_model(g_model.output)\n",
    "    # define gan model as taking noise and outputting a classification\n",
    "    model = Model(g_model.input, gan_output)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the images\n",
    "def load_real_samples(X, Y):\n",
    "    # load dataset\n",
    "    y_index = tf.argmax(Y, axis=1)\n",
    "    \n",
    "#     print(X.shape, y_index.shape)\n",
    "    return [X, y_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a supervised subset of the dataset, ensures classes are balanced\n",
    "def select_supervised_samples(dataset, n_samples=X_train.shape[0], bi_class=bi_class, n_classes=num_classes):\n",
    "    X, y = dataset\n",
    "    X_list, y_list = list(), list()\n",
    "    if bi_class != 0:\n",
    "        n_classes = 2\n",
    "    n_per_class = int(n_samples / n_classes)\n",
    "    for i in range(n_classes):\n",
    "        # get all images for this class\n",
    "        X_with_class = X[y == i]\n",
    "        # choose random instances\n",
    "        ix = np.random.randint(0, len(X_with_class), n_per_class)\n",
    "        # add to list\n",
    "        [X_list.append(X_with_class[j]) for j in ix]\n",
    "        [y_list.append(i) for j in ix]\n",
    "    return np.asarray(X_list), np.asarray(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    z_input = np.random.randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = z_input.reshape(n_samples, latent_dim)\n",
    "    return z_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "#     # choose random instances\n",
    "#     ix = np.random.randint(0, images.shape[0], n_samples)\n",
    "#     print(ix)\n",
    "    # select images and labels\n",
    "    X, labels = images, labels\n",
    "    # generate class labels\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return [X, labels], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict(z_input)\n",
    "    # create class labels\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return images, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, c_model, latent_dim, dataset, n_samples=32):\n",
    "    # evaluate the classifier model\n",
    "    X, y = dataset\n",
    "    _, acc = c_model.evaluate(X, y, verbose=0)\n",
    "    print('Classifier Accuracy: %.3f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, c_model, gan_model, X, y, latent_dim, n_epochs=50, bi_class=bi_class, n_batch=32):\n",
    "    loss_hist = []\n",
    "    acc_hist = []\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(X_train)).batch(16)\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = len(train_dataset)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # calculate the size of half a batch of samples\n",
    "    half_batch = int(n_batch / 2)\n",
    "    print('n_epochs=%d, n_batch=%d, 1/2=%d, b/e=%d, steps=%d' % (n_epochs, n_batch, half_batch, bat_per_epo, n_steps))\n",
    "    # manually enumerate epochs\n",
    "    steps = 0\n",
    "    for i in range(n_epochs):\n",
    "        start = time.time()\n",
    "        for data in train_dataset:\n",
    "            Xsup_real, ysup_real = data[0], data[1]\n",
    "            # update unsupervised classifier (c)\n",
    "            if bi_class == 0:\n",
    "                c_loss, c_acc = c_model.train_on_batch(Xsup_real, ysup_real)\n",
    "#                 c_loss, c_acc, c_auc, c_f1 = c_model.train_on_batch(Xsup_real, ysup_real)\n",
    "            else:\n",
    "                c_loss, c_acc = c_model.train_on_batch(Xsup_real, ysup_real)\n",
    "#                 c_loss, c_acc, c_auc = c_model.train_on_batch(Xsup_real, ysup_real)\n",
    "            # update unsupervised discriminator (d)\n",
    "            [X_real, _], y_real = generate_real_samples([Xsup_real, ysup_real], len(ysup_real))\n",
    "            d_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "            # update generator (g)\n",
    "            X_gan, y_gan = generate_latent_points(latent_dim, n_batch), np.ones((n_batch, 1))\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            steps += 1\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, c[%.3f,%.0f], d[%.3f,%.3f], g[%.3f]' % (steps, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
    "\n",
    "        steps=0\n",
    "        if bi_class == 0:\n",
    "            loss, acc= c_model.evaluate(X, y, verbose=0)\n",
    "            print('%d epoch Classifier Loss: %.3f Accuracy: %.3f%%' % (i+1, loss, acc * 100))\n",
    "            # print (' 에포크 {} 에서 걸린 시간은 {} 초 입니다'.format(epoch +1, time.time()-start))\n",
    "            print ('Runtime: {} sec'.format(time.time()-start))\n",
    "            loss_hist.append(loss)\n",
    "            acc_hist.append(acc)\n",
    "        else:\n",
    "            loss, acc = c_model.evaluate(X, y, verbose=0)\n",
    "            print('%d epoch Classifier Loss: %.3f Accuracy: %.3f%%' % (i+1, loss, acc * 100))\n",
    "            # print (' 에포크 {} 에서 걸린 시간은 {} 초 입니다'.format(epoch +1, time.time()-start))\n",
    "            print ('Runtime: {} sec'.format(time.time()-start))\n",
    "            loss_hist.append(loss)\n",
    "            acc_hist.append(acc)\n",
    "        \n",
    "    return loss_hist, acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((np.array(X_train).transpose([0,1,2,3]), tf.argmax(np.array(Y_train).transpose([0,1]), axis=1))).shuffle(len(X_train)).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# size of the latent space\n",
    "latent_dim = 31\n",
    "# create the discriminator models\n",
    "d_model, c_model = define_discriminator()\n",
    "# create the generator\n",
    "g_model = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# load image data\n",
    "dataset = load_real_samples(X=np.array(X_train).transpose([0,1,2,3]), Y=np.array(Y_train).transpose([0,1]))\n",
    "# train model\n",
    "loss, acc = train(g_model, d_model, c_model, gan_model, np.array(X_train).transpose([0,1,2,3]), tf.argmax(np.array(Y_train).transpose([0,1]), axis=1), latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = c_model.predict(np.array(X_train).transpose([0,1,2,3]))\n",
    "\n",
    "if bi_class==0:\n",
    "    auc = roc_auc_score(Y_train, predictions, multi_class='raise')\n",
    "    print('Multiclass Train AUC: ', auc)\n",
    "else:\n",
    "    auc = roc_auc_score(Y_train, predictions)\n",
    "    print('Train AUC: ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bi_class==0:\n",
    "    f1 = f1_score(np.argmax(Y_train, axis=1), tf.argmax(predictions,axis=1), average='macro')\n",
    "    print('Multiclass Train F1: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display acc, loss\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(loss, 'y', label='train loss')\n",
    "\n",
    "acc_ax.plot(acc, 'b', label='train acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "# acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = tf.argmax(np.array(Y_test).transpose([0,1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_acc = c_model.evaluate(np.array(X_test).transpose([0,1,2,3]),  y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = c_model.predict(np.array(X_test).transpose([0,1,2,3]))\n",
    "\n",
    "if bi_class==0:\n",
    "    auc = roc_auc_score(Y_test, predictions, multi_class='raise')\n",
    "    print('Multiclass Train AUC: ', auc)\n",
    "else:\n",
    "    auc = roc_auc_score(Y_test, predictions)\n",
    "    print('Train AUC: ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bi_class==0:\n",
    "    f1 = f1_score(np.argmax(Y_test, axis=1), tf.argmax(predictions,axis=1), average='macro')\n",
    "    print('Multiclass Train F1: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
