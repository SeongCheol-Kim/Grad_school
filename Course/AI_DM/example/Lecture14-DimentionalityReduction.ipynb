{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDH_d7IBWxi2"
   },
   "source": [
    "# Dimensionality Reduction: Geometric view of data\n",
    "\n",
    "__Content creators:__ Alex Cayco Gajic, John Murray\n",
    "\n",
    "__Content reviewers:__ Roozbeh Farhoudi, Matt Krause, Spiros Chavlis, Richard Gao, Michael Waskom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL4Brd14Wxi9"
   },
   "source": [
    "---\n",
    "# Tutorial1- Objectives\n",
    "\n",
    "In this notebook we'll explore how multivariate data can be represented in different orthonormal bases. This will help us build intuition that will be helpful in understanding PCA in the following tutorial. \n",
    "\n",
    "Overview:\n",
    " - Generate correlated multivariate data.\n",
    " - Define an arbitrary orthonormal basis. \n",
    " - Project the data onto the new basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "vF7HDJt6Wxi-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video available at https://youtube.com/watch?v=THu9yHnpq9I\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"854\"\n",
       "            height=\"480\"\n",
       "            src=\"https://www.youtube.com/embed/THu9yHnpq9I?fs=1\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7fb5c82d4050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Video 1: Geometric view of data\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"THu9yHnpq9I\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reagjtUMWxjA"
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J27coixCWxjA"
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "RYLN1KjoWxjB"
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets  # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "yFMO3sTXWxjB"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "\n",
    "def get_data(cov_matrix):\n",
    "  \"\"\"\n",
    "  Returns a matrix of 1000 samples from a bivariate, zero-mean Gaussian.\n",
    "\n",
    "  Note that samples are sorted in ascending order for the first random variable\n",
    "\n",
    "  Args:\n",
    "    cov_matrix (numpy array of floats): desired covariance matrix\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats) : samples from the bivariate Gaussian, with each\n",
    "                              column corresponding to a different random\n",
    "                              variable\n",
    "  \"\"\"\n",
    "\n",
    "  mean = np.array([0, 0])\n",
    "  X = np.random.multivariate_normal(mean, cov_matrix, size=1000)\n",
    "  indices_for_sorting = np.argsort(X[:, 0])\n",
    "  X = X[indices_for_sorting, :]\n",
    "\n",
    "  return X\n",
    "\n",
    "\n",
    "def plot_data(X):\n",
    "  \"\"\"\n",
    "  Plots bivariate data. Includes a plot of each random variable, and a scatter\n",
    "  plot of their joint activity. The title indicates the sample correlation\n",
    "  calculated from the data.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) :   Data matrix each column corresponds to a\n",
    "                                  different random variable\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  fig = plt.figure(figsize=[8, 4])\n",
    "  gs = fig.add_gridspec(2, 2)\n",
    "  ax1 = fig.add_subplot(gs[0, 0])\n",
    "  ax1.plot(X[:, 0], color='k')\n",
    "  plt.ylabel('Neuron 1')\n",
    "  plt.title('Sample var 1: {:.1f}'.format(np.var(X[:, 0])))\n",
    "  ax1.set_xticklabels([])\n",
    "  ax2 = fig.add_subplot(gs[1, 0])\n",
    "  ax2.plot(X[:, 1], color='k')\n",
    "  plt.xlabel('Sample Number')\n",
    "  plt.ylabel('Neuron 2')\n",
    "  plt.title('Sample var 2: {:.1f}'.format(np.var(X[:, 1])))\n",
    "  ax3 = fig.add_subplot(gs[:, 1])\n",
    "  ax3.plot(X[:, 0], X[:, 1], '.', markerfacecolor=[.5, .5, .5],\n",
    "           markeredgewidth=0)\n",
    "  ax3.axis('equal')\n",
    "  plt.xlabel('Neuron 1 activity')\n",
    "  plt.ylabel('Neuron 2 activity')\n",
    "  plt.title('Sample corr: {:.1f}'.format(np.corrcoef(X[:, 0], X[:, 1])[0, 1]))\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_basis_vectors(X, W):\n",
    "  \"\"\"\n",
    "  Plots bivariate data as well as new basis vectors.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) :   Data matrix each column corresponds to a\n",
    "                                  different random variable\n",
    "    W (numpy array of floats) :   Square matrix representing new orthonormal\n",
    "                                  basis each column represents a basis vector\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure(figsize=[4, 4])\n",
    "  plt.plot(X[:, 0], X[:, 1], '.', color=[.5, .5, .5], label='Data')\n",
    "  plt.axis('equal')\n",
    "  plt.xlabel('Neuron 1 activity')\n",
    "  plt.ylabel('Neuron 2 activity')\n",
    "  plt.plot([0, W[0, 0]], [0, W[1, 0]], color='r', linewidth=3,\n",
    "           label='Basis vector 1')\n",
    "  plt.plot([0, W[0, 1]], [0, W[1, 1]], color='b', linewidth=3,\n",
    "           label='Basis vector 2')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_data_new_basis(Y):\n",
    "  \"\"\"\n",
    "  Plots bivariate data after transformation to new bases.\n",
    "  Similar to plot_data but with colors corresponding to projections onto\n",
    "  basis 1 (red) and basis 2 (blue). The title indicates the sample correlation\n",
    "  calculated from the data.\n",
    "\n",
    "  Note that samples are re-sorted in ascending order for the first\n",
    "  random variable.\n",
    "\n",
    "  Args:\n",
    "    Y (numpy array of floats):   Data matrix in new basis each column\n",
    "                                 corresponds to a different random variable\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  fig = plt.figure(figsize=[8, 4])\n",
    "  gs = fig.add_gridspec(2, 2)\n",
    "  ax1 = fig.add_subplot(gs[0, 0])\n",
    "  ax1.plot(Y[:, 0], 'r')\n",
    "  plt.xlabel\n",
    "  plt.ylabel('Projection \\n basis vector 1')\n",
    "  plt.title('Sample var 1: {:.1f}'.format(np.var(Y[:, 0])))\n",
    "  ax1.set_xticklabels([])\n",
    "  ax2 = fig.add_subplot(gs[1, 0])\n",
    "  ax2.plot(Y[:, 1], 'b')\n",
    "  plt.xlabel('Sample number')\n",
    "  plt.ylabel('Projection \\n basis vector 2')\n",
    "  plt.title('Sample var 2: {:.1f}'.format(np.var(Y[:, 1])))\n",
    "  ax3 = fig.add_subplot(gs[:, 1])\n",
    "  ax3.plot(Y[:, 0], Y[:, 1], '.', color=[.5, .5, .5])\n",
    "  ax3.axis('equal')\n",
    "  plt.xlabel('Projection basis vector 1')\n",
    "  plt.ylabel('Projection basis vector 2')\n",
    "  plt.title('Sample corr: {:.1f}'.format(np.corrcoef(Y[:, 0], Y[:, 1])[0, 1]))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6QkhZ67WxjC"
   },
   "source": [
    "---\n",
    "# Section 1: Generate correlated multivariate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qFLXwHaKWxjC"
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Multivariate data\n",
    "video = YouTubeVideo(id=\"jcTq2PgU5Vw\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "payVf9n-WxjD"
   },
   "source": [
    "To gain intuition, we will first use a simple model to generate multivariate data. Specifically, we will draw random samples from a *bivariate normal distribution*. This is an extension of the one-dimensional normal distribution to two dimensions, in which each $x_i$ is marginally normal with mean $\\mu_i$ and variance $\\sigma_i^2$:\n",
    "\n",
    "\\begin{align}\n",
    "x_i \\sim \\mathcal{N}(\\mu_i,\\sigma_i^2).\n",
    "\\end{align}\n",
    "\n",
    "Additionally, the joint distribution for $x_1$ and $x_2$ has a specified correlation coefficient $\\rho$. Recall that the correlation coefficient is a normalized version of the covariance, and ranges between -1 and +1:\n",
    "\n",
    "\\begin{align}\n",
    "\\rho = \\frac{\\text{cov}(x_1,x_2)}{\\sqrt{\\sigma_1^2 \\sigma_2^2}}.\n",
    "\\end{align}\n",
    "\n",
    "For simplicity, we will assume that the mean of each variable has already been subtracted, so that $\\mu_i=0$. The remaining parameters can be summarized in the covariance matrix, which for two dimensions has the following form:\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\bf \\Sigma} = \n",
    "\\begin{pmatrix}\n",
    " \\text{var}(x_1) & \\text{cov}(x_1,x_2) \\\\\n",
    " \\text{cov}(x_1,x_2) &\\text{var}(x_2)\n",
    "\\end{pmatrix}.\n",
    "\\end{equation*}\n",
    "\n",
    "In general, $\\bf \\Sigma$ is a symmetric matrix with the variances $\\text{var}(x_i) = \\sigma_i^2$ on the diagonal, and the covariances on the off-diagonal. Later, we will see that the covariance matrix plays a key role in PCA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HONtKncWxjD"
   },
   "source": [
    "\n",
    "## Exercise 1: Draw samples from a distribution\n",
    "\n",
    "We have provided code to draw random samples from a zero-mean bivariate normal distribution. Throughout this tutorial, we'll imagine these samples represent the activity (firing rates) of two recorded neurons on different trials. Fill in the function below to calculate the covariance matrix given the desired variances and correlation coefficient. The covariance can be found by rearranging the equation above:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{cov}(x_1,x_2) = \\rho \\sqrt{\\sigma_1^2 \\sigma_2^2}.\n",
    "\\end{align}\n",
    "\n",
    "Use these functions to generate and plot data while varying the parameters. You should get a feel for how changing the correlation coefficient affects the geometry of the simulated data.\n",
    "\n",
    "**Steps**\n",
    "* Fill in the function `calculate_cov_matrix` to calculate the desired covariance.\n",
    "* Generate and plot the data for $\\sigma_1^2 =1$, $\\sigma_1^2 =1$, and $\\rho = .8$. Try plotting the data for different values of the correlation coefficent: $\\rho = -1, -.5, 0, .5, 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkFpiqHQWxjF"
   },
   "outputs": [],
   "source": [
    "help(plot_data)\n",
    "help(get_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOOto7FUWxjF"
   },
   "outputs": [],
   "source": [
    "def calculate_cov_matrix(var_1, var_2, corr_coef):\n",
    "  \"\"\"\n",
    "  Calculates the covariance matrix based on the variances and correlation\n",
    "  coefficient.\n",
    "\n",
    "  Args:\n",
    "    var_1 (scalar)          : variance of the first random variable\n",
    "    var_2 (scalar)          : variance of the second random variable\n",
    "    corr_coef (scalar)      : correlation coefficient\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats) : covariance matrix\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  ## TODO for students: calculate the covariance matrix\n",
    "  # Fill out function and remove\n",
    "  raise NotImplementedError(\"Student excercise: calculate the covariance matrix!\")\n",
    "  #################################################\n",
    "\n",
    "  # Calculate the covariance from the variances and correlation\n",
    "  cov = ...\n",
    "\n",
    "  cov_matrix = np.array([[var_1, cov], [cov, var_2]])\n",
    "\n",
    "  return cov_matrix\n",
    "\n",
    "\n",
    "###################################################################\n",
    "## TO DO for students: generate and plot bivariate Gaussian data with variances of 1\n",
    "## and a correlation coefficients of: 0.8\n",
    "## repeat while varying the correlation coefficient from -1 to 1\n",
    "###################################################################\n",
    "np.random.seed(2020)  # set random seed\n",
    "variance_1 = 1\n",
    "variance_2 = 1\n",
    "corr_coef = 0.8\n",
    "\n",
    "# Uncomment to test your code and plot\n",
    "# cov_matrix = calculate_cov_matrix(variance_1, variance_2, corr_coef)\n",
    "# X = get_data(cov_matrix)\n",
    "# plot_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1VYL45MWxjG"
   },
   "source": [
    "---\n",
    "# Section 2: Define a new orthonormal basis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "M52fmQGLWxjG"
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Orthonormal bases\n",
    "video = YouTubeVideo(id=\"PC1RZELnrIg\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyjFEJAlWxjH"
   },
   "source": [
    "Next, we will define a new orthonormal basis of vectors ${\\bf u} = [u_1,u_2]$ and ${\\bf w} = [w_1,w_2]$. As we learned in the video, two vectors are orthonormal if: \n",
    "\n",
    "1.   They are orthogonal (i.e., their dot product is zero):\n",
    "\\begin{equation}\n",
    "{\\bf u\\cdot w} = u_1 w_1 + u_2 w_2 = 0\n",
    "\\end{equation}\n",
    "2.   They have unit length:\n",
    "\\begin{equation}\n",
    "||{\\bf u} || = ||{\\bf w} || = 1\n",
    "\\end{equation}\n",
    "\n",
    "In two dimensions, it is easy to make an arbitrary orthonormal basis. All we need is a random vector ${\\bf u}$, which we have normalized. If we now define the second basis vector to be ${\\bf w} = [-u_2,u_1]$, we can check that both conditions are satisfied: \n",
    "\\begin{equation}\n",
    "{\\bf u\\cdot w} = - u_1 u_2 + u_2 u_1 = 0\n",
    "\\end{equation}\n",
    "and \n",
    "\\begin{equation}\n",
    "{|| {\\bf w} ||} = \\sqrt{(-u_2)^2 + u_1^2} = \\sqrt{u_1^2 + u_2^2} = 1,\n",
    "\\end{equation}\n",
    "where we used the fact that ${\\bf u}$ is normalized. So, with an arbitrary input vector, we can define an orthonormal basis, which we will write in matrix by stacking the basis vectors horizontally:\n",
    "\n",
    "\\begin{equation}\n",
    "{{\\bf W} } =\n",
    "\\begin{pmatrix}\n",
    " u_1 & w_1 \\\\\n",
    " u_2 & w_2\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNWQ3NpYWxjH"
   },
   "source": [
    "## Exercise 2: Find an orthonormal basis\n",
    "\n",
    "In this exercise you will fill in the function below to define an orthonormal basis, given a single arbitrary 2-dimensional vector as an input.\n",
    "\n",
    "**Steps**\n",
    "* Modify the function `define_orthonormal_basis` to first normalize the first basis vector $\\bf u$.\n",
    "* Then complete the function by finding a basis vector $\\bf w$ that is orthogonal to $\\bf u$.\n",
    "* Test the function using initial basis vector ${\\bf u} = [3,1]$. Plot the resulting basis vectors on top of the data scatter plot using the function `plot_basis_vectors`. (For the data, use  $\\sigma_1^2 =1$, $\\sigma_2^2 =1$, and $\\rho = .8$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRx1YtVqWxjH"
   },
   "outputs": [],
   "source": [
    "help(plot_basis_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pL_vmuGJWxjI"
   },
   "outputs": [],
   "source": [
    "def define_orthonormal_basis(u):\n",
    "  \"\"\"\n",
    "  Calculates an orthonormal basis given an arbitrary vector u.\n",
    "\n",
    "  Args:\n",
    "    u (numpy array of floats) : arbitrary 2-dimensional vector used for new\n",
    "                                basis\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : new orthonormal basis\n",
    "                                columns correspond to basis vectors\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  ## TODO for students: calculate the orthonormal basis\n",
    "  # Fill out function and remove\n",
    "  raise NotImplementedError(\"Student excercise: implement the orthonormal basis function\")\n",
    "  #################################################\n",
    "\n",
    "  # normalize vector u\n",
    "  u = ...\n",
    "  # calculate vector w that is orthogonal to w\n",
    "  w = ...\n",
    "\n",
    "  W = np.column_stack([u, w])\n",
    "\n",
    "  return W\n",
    "\n",
    "\n",
    "np.random.seed(2020)  # set random seed\n",
    "variance_1 = 1\n",
    "variance_2 = 1\n",
    "corr_coef = 0.8\n",
    "\n",
    "cov_matrix = calculate_cov_matrix(variance_1, variance_2, corr_coef)\n",
    "X = get_data(cov_matrix)\n",
    "u = np.array([3, 1])\n",
    "\n",
    "# Uncomment and run below to plot the basis vectors\n",
    "# W = define_orthonormal_basis(u)\n",
    "# plot_basis_vectors(X, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqUAB31RWxjI"
   },
   "source": [
    "---\n",
    "# Section 3: Project data onto new basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "T_n7kDypWxjI"
   },
   "outputs": [],
   "source": [
    "# @title Video 4: Change of basis\n",
    "video = YouTubeVideo(id=\"Mj6BRQPKKUc\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neJtZRNqWxjJ"
   },
   "source": [
    "\n",
    "   \n",
    "Finally, we will express our data in the new basis that we have just found. Since $\\bf W$ is orthonormal, we can project the data into our new basis using simple matrix multiplication :\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf Y = X W}.\n",
    "\\end{equation}\n",
    "\n",
    "We will explore the geometry of the transformed data $\\bf Y$ as we vary the choice of basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h763BNqWxjJ"
   },
   "source": [
    "## Exercise 3: Define an orthonormal basis\n",
    "In this exercise you will fill in the function below to define an orthonormal basis, given a single arbitrary vector as an input.\n",
    "\n",
    "**Steps**\n",
    "* Complete the function `change_of_basis` to project the data onto the new basis.\n",
    "* Plot the projected data using the function `plot_data_new_basis`. \n",
    "* What happens to the correlation coefficient in the new basis? Does it increase or decrease? \n",
    "* What happens to variance? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sy8DjzDhWxjJ"
   },
   "outputs": [],
   "source": [
    "def change_of_basis(X, W):\n",
    "  \"\"\"\n",
    "  Projects data onto new basis W.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponding to a\n",
    "                                different random variable\n",
    "    W (numpy array of floats) : new orthonormal basis columns correspond to\n",
    "                                basis vectors\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)    : Data matrix expressed in new basis\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  ## TODO for students: project the data onto o new basis W\n",
    "  # Fill out function and remove\n",
    "  raise NotImplementedError(\"Student excercise: implement change of basis\")\n",
    "  #################################################\n",
    "\n",
    "  # project data onto new basis described by W\n",
    "  Y = ...\n",
    "\n",
    "  return Y\n",
    "\n",
    "\n",
    "# Unomment below to transform the data by projecting it into the new basis\n",
    "# Y = change_of_basis(X, W)\n",
    "# plot_data_new_basis(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3Knrh5mWxjK"
   },
   "source": [
    "## Interactive Demo: Play with the basis vectors\n",
    "To see what happens to the correlation as we change the basis vectors, run the cell below. The parameter $\\theta$ controls the angle of $\\bf u$ in degrees. Use the slider to rotate the basis vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bAyEVGg6WxjK"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\n",
    "# @markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "\n",
    "def refresh(theta=0):\n",
    "  u = [1, np.tan(theta * np.pi / 180)]\n",
    "  W = define_orthonormal_basis(u)\n",
    "  Y = change_of_basis(X, W)\n",
    "  plot_basis_vectors(X, W)\n",
    "  plot_data_new_basis(Y)\n",
    "\n",
    "\n",
    "_ = widgets.interact(refresh, theta=(0, 90, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJs-T267WxjK"
   },
   "source": [
    "## Questions\n",
    "\n",
    "* What happens to the projected data as you rotate the basis? \n",
    "* How does the correlation coefficient change? How does the variance of the projection onto each basis vector change?\n",
    "* Are you able to find a basis in which the projected data is **uncorrelated**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MuwHwdoWxjK"
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "- In this tutorial, we learned that multivariate data can be visualized as a cloud of points in a high-dimensional vector space. The geometry of this cloud is shaped by the covariance matrix.\n",
    "\n",
    "- Multivariate data can be represented in a new orthonormal basis using the dot product. These new basis vectors correspond to specific mixtures of the original variables - for example, in neuroscience, they could represent different ratios of activation  across a population of neurons.\n",
    "\n",
    "- The projected data (after transforming into the new basis) will generally have a different geometry from the original data. In particular, taking basis vectors that are aligned with the spread of cloud of points decorrelates the data.\n",
    "\n",
    "* These concepts - covariance, projections, and orthonormal bases - are key for understanding PCA, which we be our focus in the next tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_s5HGDiXCLl"
   },
   "source": [
    "# Dimensionality Reduction: Principal component analysis\n",
    "\n",
    "__Content creators:__ Alex Cayco Gajic, John Murray\n",
    "\n",
    "__Content reviewers:__ Roozbeh Farhoudi, Matt Krause, Spiros Chavlis, Richard Gao, Michael Waskom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxzM9gUDXCLr"
   },
   "source": [
    "---\n",
    "# Tutorial 2- Objectives\n",
    "\n",
    "In this notebook we'll learn how to perform PCA by projecting the data onto the eigenvectors of its covariance matrix. \n",
    "\n",
    "Overview:\n",
    "- Calculate the eigenvectors of the sample covariance matrix.\n",
    "- Perform PCA by projecting data onto the eigenvectors of the covariance matrix. \n",
    "- Plot and explore the eigenvalues.\n",
    "\n",
    "To quickly refresh your knowledge of eigenvalues and eigenvectors, you can watch this [short video](https://www.youtube.com/watch?v=kwA3qM0rm7c) (4 minutes) for a geometrical explanation. For a deeper understanding, this [in-depth video](https://www.youtube.com/watch?v=PFDu9oVAE-g&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=14) (17 minutes) provides an excellent basis and is beautifully illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "91jl3TlhXCLs"
   },
   "outputs": [],
   "source": [
    "# @title Video 1: PCA\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"-f6T9--oM0E\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DVqtor_XCLt"
   },
   "source": [
    "---\n",
    "# Setup\n",
    "Run these cells to get the tutorial started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHiO341wXCLt"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pFi9A_G5XCLt"
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets  # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kizMDa5gXCLu"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "\n",
    "def plot_eigenvalues(evals):\n",
    "  \"\"\"\n",
    "  Plots eigenvalues.\n",
    "\n",
    "  Args:\n",
    "      (numpy array of floats) : Vector of eigenvalues\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(4, 4))\n",
    "  plt.plot(np.arange(1, len(evals) + 1), evals, 'o-k')\n",
    "  plt.xlabel('Component')\n",
    "  plt.ylabel('Eigenvalue')\n",
    "  plt.title('Scree plot')\n",
    "  plt.xticks(np.arange(1, len(evals) + 1))\n",
    "  plt.ylim([0, 2.5])\n",
    "\n",
    "\n",
    "def sort_evals_descending(evals, evectors):\n",
    "  \"\"\"\n",
    "  Sorts eigenvalues and eigenvectors in decreasing order. Also aligns first two\n",
    "  eigenvectors to be in first two quadrants (if 2D).\n",
    "\n",
    "  Args:\n",
    "    evals (numpy array of floats)    : Vector of eigenvalues\n",
    "    evectors (numpy array of floats) : Corresponding matrix of eigenvectors\n",
    "                                        each column corresponds to a different\n",
    "                                        eigenvalue\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)          : Vector of eigenvalues after sorting\n",
    "    (numpy array of floats)          : Matrix of eigenvectors after sorting\n",
    "  \"\"\"\n",
    "\n",
    "  index = np.flip(np.argsort(evals))\n",
    "  evals = evals[index]\n",
    "  evectors = evectors[:, index]\n",
    "  if evals.shape[0] == 2:\n",
    "    if np.arccos(np.matmul(evectors[:, 0],\n",
    "                           1 / np.sqrt(2) * np.array([1, 1]))) > np.pi / 2:\n",
    "      evectors[:, 0] = -evectors[:, 0]\n",
    "    if np.arccos(np.matmul(evectors[:, 1],\n",
    "                           1 / np.sqrt(2) * np.array([-1, 1]))) > np.pi / 2:\n",
    "      evectors[:, 1] = -evectors[:, 1]\n",
    "  return evals, evectors\n",
    "\n",
    "\n",
    "def plot_data(X):\n",
    "  \"\"\"\n",
    "  Plots bivariate data. Includes a plot of each random variable, and a scatter\n",
    "  scatter plot of their joint activity. The title indicates the sample\n",
    "  correlation calculated from the data.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
    "                                different random variable\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  fig = plt.figure(figsize=[8, 4])\n",
    "  gs = fig.add_gridspec(2, 2)\n",
    "  ax1 = fig.add_subplot(gs[0, 0])\n",
    "  ax1.plot(X[:, 0], color='k')\n",
    "  plt.ylabel('Neuron 1')\n",
    "  ax2 = fig.add_subplot(gs[1, 0])\n",
    "  ax2.plot(X[:, 1], color='k')\n",
    "  plt.xlabel('Sample Number (sorted)')\n",
    "  plt.ylabel('Neuron 2')\n",
    "  ax3 = fig.add_subplot(gs[:, 1])\n",
    "  ax3.plot(X[:, 0], X[:, 1], '.', markerfacecolor=[.5, .5, .5],\n",
    "           markeredgewidth=0)\n",
    "  ax3.axis('equal')\n",
    "  plt.xlabel('Neuron 1 activity')\n",
    "  plt.ylabel('Neuron 2 activity')\n",
    "  plt.title('Sample corr: {:.1f}'.format(np.corrcoef(X[:, 0], X[:, 1])[0, 1]))\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def get_data(cov_matrix):\n",
    "  \"\"\"\n",
    "  Returns a matrix of 1000 samples from a bivariate, zero-mean Gaussian\n",
    "\n",
    "  Note that samples are sorted in ascending order for the first random\n",
    "  variable.\n",
    "\n",
    "  Args:\n",
    "    var_1 (scalar)                     : variance of the first random variable\n",
    "    var_2 (scalar)                     : variance of the second random variable\n",
    "    cov_matrix (numpy array of floats) : desired covariance matrix\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)            : samples from the bivariate Gaussian,\n",
    "                                          with each column corresponding to a\n",
    "                                          different random variable\n",
    "  \"\"\"\n",
    "\n",
    "  mean = np.array([0, 0])\n",
    "  X = np.random.multivariate_normal(mean, cov_matrix, size=1000)\n",
    "  indices_for_sorting = np.argsort(X[:, 0])\n",
    "  X = X[indices_for_sorting, :]\n",
    "  return X\n",
    "\n",
    "\n",
    "def calculate_cov_matrix(var_1, var_2, corr_coef):\n",
    "  \"\"\"\n",
    "  Calculates the covariance matrix based on the variances and\n",
    "  correlation coefficient.\n",
    "\n",
    "  Args:\n",
    "    var_1 (scalar)         :  variance of the first random variable\n",
    "    var_2 (scalar)         :  variance of the second random variable\n",
    "    corr_coef (scalar)     :  correlation coefficient\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats) : covariance matrix\n",
    "  \"\"\"\n",
    "  cov = corr_coef * np.sqrt(var_1 * var_2)\n",
    "  cov_matrix = np.array([[var_1, cov], [cov, var_2]])\n",
    "  return cov_matrix\n",
    "\n",
    "\n",
    "def define_orthonormal_basis(u):\n",
    "  \"\"\"\n",
    "  Calculates an orthonormal basis given an arbitrary vector u.\n",
    "\n",
    "  Args:\n",
    "    u (numpy array of floats) : arbitrary 2D vector used for new basis\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : new orthonormal basis columns correspond to\n",
    "                                basis vectors\n",
    "  \"\"\"\n",
    "\n",
    "  u = u / np.sqrt(u[0] ** 2 + u[1] ** 2)\n",
    "  w = np.array([-u[1], u[0]])\n",
    "  W = np.column_stack((u, w))\n",
    "  return W\n",
    "\n",
    "\n",
    "def plot_data_new_basis(Y):\n",
    "  \"\"\"\n",
    "  Plots bivariate data after transformation to new bases. Similar to plot_data\n",
    "  but with colors corresponding to projections onto basis 1 (red) and\n",
    "  basis 2 (blue).\n",
    "  The title indicates the sample correlation calculated from the data.\n",
    "\n",
    "  Note that samples are re-sorted in ascending order for the first random\n",
    "  variable.\n",
    "\n",
    "  Args:\n",
    "    Y (numpy array of floats) : Data matrix in new basis each column\n",
    "                                corresponds to a different random variable\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  fig = plt.figure(figsize=[8, 4])\n",
    "  gs = fig.add_gridspec(2, 2)\n",
    "  ax1 = fig.add_subplot(gs[0, 0])\n",
    "  ax1.plot(Y[:, 0], 'r')\n",
    "  plt.ylabel('Projection \\n basis vector 1')\n",
    "  ax2 = fig.add_subplot(gs[1, 0])\n",
    "  ax2.plot(Y[:, 1], 'b')\n",
    "  plt.xlabel('Sample number')\n",
    "  plt.ylabel('Projection \\n basis vector 2')\n",
    "  ax3 = fig.add_subplot(gs[:, 1])\n",
    "  ax3.plot(Y[:, 0], Y[:, 1], '.', color=[.5, .5, .5])\n",
    "  ax3.axis('equal')\n",
    "  plt.xlabel('Projection basis vector 1')\n",
    "  plt.ylabel('Projection basis vector 2')\n",
    "  plt.title('Sample corr: {:.1f}'.format(np.corrcoef(Y[:, 0], Y[:, 1])[0, 1]))\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def change_of_basis(X, W):\n",
    "  \"\"\"\n",
    "  Projects data onto a new basis.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponding to a\n",
    "                                different random variable\n",
    "    W (numpy array of floats) : new orthonormal basis columns correspond to\n",
    "                                basis vectors\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : Data matrix expressed in new basis\n",
    "  \"\"\"\n",
    "\n",
    "  Y = np.matmul(X, W)\n",
    "  return Y\n",
    "\n",
    "\n",
    "def plot_basis_vectors(X, W):\n",
    "  \"\"\"\n",
    "  Plots bivariate data as well as new basis vectors.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
    "                                different random variable\n",
    "    W (numpy array of floats) : Square matrix representing new orthonormal\n",
    "                                basis each column represents a basis vector\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure(figsize=[4, 4])\n",
    "  plt.plot(X[:, 0], X[:, 1], '.', color=[.5, .5, .5], label='Data')\n",
    "  plt.axis('equal')\n",
    "  plt.xlabel('Neuron 1 activity')\n",
    "  plt.ylabel('Neuron 2 activity')\n",
    "  plt.plot([0, W[0, 0]], [0, W[1, 0]], color='r', linewidth=3,\n",
    "           label='Basis vector 1')\n",
    "  plt.plot([0, W[0, 1]], [0, W[1, 1]], color='b', linewidth=3,\n",
    "           label='Basis vector 2')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_3t805NXCLx"
   },
   "source": [
    "---\n",
    "# Section 1: Calculate the eigenvectors of the the sample covariance matrix\n",
    "\n",
    "As we saw in the lecture, PCA represents data in a new orthonormal basis defined by the eigenvectors of the covariance matrix. Remember that in the previous tutorial, we generated bivariate normal data with a specified covariance matrix $\\bf \\Sigma$, whose $(i,j)$th element is:\n",
    "\\begin{equation}\n",
    "\\Sigma_{ij} = E[ x_i x_j ] - E[ x_i] E[ x_j ] .\n",
    "\\end{equation}\n",
    "However, in real life we don't have access to this ground-truth covariance matrix. To get around this, we can use the sample covariance matrix, $\\bf\\hat\\Sigma$, which is calculated directly from the data. The $(i,j)$th element of the sample covariance matrix is:\n",
    "\\begin{equation}\n",
    " \\hat \\Sigma_{ij} =  \\frac{1}{N_\\text{samples}}{\\bf x}_i^T {\\bf x}_j - \\bar {\\bf x}_i \\bar{\\bf x}_j ,\n",
    "\\end{equation}\n",
    "where ${\\bf x}_i = [ x_i(1), x_i(2), \\dots,x_i(N_\\text{samples})]^T$ is a column vector representing all measurements of neuron $i$, and  $\\bar {\\bf x}_i$ is the mean of neuron $i$ across samples:\n",
    "\\begin{equation}\n",
    "\\bar {\\bf x}_i = \\frac{1}{N_\\text{samples}} \\sum_{k=1}^{N_\\text{samples}} x_i(k).\n",
    "\\end{equation}\n",
    "If we assume that the data has already been mean-subtracted, then we can write the sample covariance matrix in a much simpler matrix form:\n",
    "\\begin{align}\n",
    "{\\bf \\hat \\Sigma}\n",
    "&= \\frac{1}{N_\\text{samples}} {\\bf X}^T {\\bf X}.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjdjAWJ9XCL0"
   },
   "source": [
    "## Exercise 1: Calculation of the covariance matrix\n",
    "\n",
    "Before calculating the eigenvectors, you must first calculate the sample covariance matrix.\n",
    "\n",
    "**Steps**\n",
    "* Complete the function `get_sample_cov_matrix` by first subtracting the sample mean of the data, then calculate $\\bf \\hat \\Sigma$ using the equation above.\n",
    "* Use `get_data` to generate bivariate normal data, and calculate the sample covariance matrix with your finished `get_sample_cov_matrix`. Compare this estimate to the true covariate matrix using `calculate_cov_matrix`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAAX9TjcXCL0"
   },
   "outputs": [],
   "source": [
    "help(get_data)\n",
    "help(calculate_cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMKco9p6XCL1"
   },
   "outputs": [],
   "source": [
    "def get_sample_cov_matrix(X):\n",
    "  \"\"\"\n",
    "  Returns the sample covariance matrix of data X\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
    "                                different random variable\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : Covariance matrix\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  ## TODO for students: calculate the covariance matrix\n",
    "  # Fill out function and remove\n",
    "  raise NotImplementedError(\"Student excercise: calculate the covariance matrix!\")\n",
    "  #################################################\n",
    "\n",
    "  # Subtract the mean of X\n",
    "  X = ...\n",
    "  # Calculate the covariance matrix (hint: use np.matmul)\n",
    "  cov_matrix = ...\n",
    "\n",
    "  return cov_matrix\n",
    "\n",
    "\n",
    "##########################################################\n",
    "## TODO for students: generate bivariate Gaussian data\n",
    "# with variances of 1 and a correlation coefficient of 0.8\n",
    "# compare the true and sample covariance matrices\n",
    "##########################################################\n",
    "variance_1 = 1\n",
    "variance_2 = 1\n",
    "corr_coef = 0.8\n",
    "\n",
    "np.random.seed(2020)  # set random seed\n",
    "# Uncomment below code to test your function\n",
    "# cov_matrix = calculate_cov_matrix(variance_1, variance_2, corr_coef)\n",
    "# print(cov_matrix)\n",
    "\n",
    "# X = get_data(cov_matrix)\n",
    "# sample_cov_matrix = get_sample_cov_matrix(X)\n",
    "# print(sample_cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvqB1iifXCL1"
   },
   "source": [
    "## Exercise 2: Eigenvectors of the Covariance matrix\n",
    "\n",
    "Next you will calculate the eigenvectors of the covariance matrix. Plot them along with the data to check that they align with the geometry of the data.\n",
    "\n",
    "**Steps:**\n",
    "* Calculate the eigenvalues and eigenvectors of the sample covariance matrix. (**Hint:** use `np.linalg.eigh`, which finds the eigenvalues of a symmetric matrix).\n",
    "* Use the provided code to sort the eigenvalues in descending order.\n",
    "* Plot the eigenvectors on a scatter plot of the data, using the function `plot_basis_vectors`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsuGnrR9XCL2"
   },
   "outputs": [],
   "source": [
    "help(sort_evals_descending)\n",
    "help(plot_basis_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiqvA84MXCL2"
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TO DO for students: Calculate and sort the eigenvalues in descending order\n",
    "#################################################\n",
    "# Calculate the eigenvalues and eigenvectors\n",
    "# evals, evectors = ...\n",
    "# Sort the eigenvalues in descending order\n",
    "# evals, evectors = ...\n",
    "\n",
    "# plot_basis_vectors(X, evectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJh2N1lfXCL3"
   },
   "source": [
    "---\n",
    "# Section 2: Perform PCA by projecting data onto the eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrMedyM3XCL3"
   },
   "source": [
    "To perform PCA, we will project the data onto the eigenvectors of the covariance matrix, i.e.:\n",
    "\\begin{equation}\n",
    "\\bf S = X W\n",
    "\\end{equation}\n",
    "where $\\bf S$ is an $N_\\text{samples} \\times N$ matrix representing the projected data (also called *scores*), and $W$ is an $N\\times N$ orthogonal matrix, each of whose columns represents the eigenvectors of the covariance matrix (also called *weights* or *loadings*). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3AX61TEXCL3"
   },
   "source": [
    "## Exercise 3: PCA implementation\n",
    "\n",
    "You will now perform PCA on the data using the intuition you have developed so far. Fill in the function below to carry out the steps to perform PCA by projecting the data onto the eigenvectors of its covariance matrix.\n",
    "\n",
    "**Steps:**\n",
    "* First subtract the mean.\n",
    "* Then calculate the sample covariance matrix.\n",
    "* Then find the eigenvalues and eigenvectors and sort them in descending order.\n",
    "* Finally project the mean-centered data onto the eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dTqRs6KXCL3"
   },
   "outputs": [],
   "source": [
    "help(change_of_basis)\n",
    "help(plot_data_new_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peiBTvUCXCL3"
   },
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "  \"\"\"\n",
    "  Sorts eigenvalues and eigenvectors in decreasing order.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats): Data matrix each column corresponds to a\n",
    "                               different random variable\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)  : Data projected onto the new basis\n",
    "    (numpy array of floats)  : Vector of eigenvalues\n",
    "    (numpy array of floats)  : Corresponding matrix of eigenvectors\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  ## TODO for students: calculate the covariance matrix\n",
    "  # Fill out function and remove\n",
    "  raise NotImplementedError(\"Student excercise: sort eigenvalues/eigenvectors!\")\n",
    "  #################################################\n",
    "\n",
    "  # Subtract the mean of X\n",
    "  X = ...\n",
    "  # Calculate the sample covariance matrix\n",
    "  cov_matrix = ...\n",
    "  # Calculate the eigenvalues and eigenvectors\n",
    "  evals, evectors = ...\n",
    "  # Sort the eigenvalues in descending order\n",
    "  evals, evectors = ...\n",
    "  # Project the data onto the new eigenvector basis\n",
    "  score = ...\n",
    "\n",
    "  return score, evectors, evals\n",
    "\n",
    "\n",
    "#################################################\n",
    "## TODO for students: Call the function to calculate the eigenvectors/eigenvalues\n",
    "#################################################\n",
    "\n",
    "# Perform PCA on the data matrix X\n",
    "# score, evectors, evals = ...\n",
    "# Plot the data projected into the new basis\n",
    "# plot_data_new_basis(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DWd7usLXCL3"
   },
   "source": [
    "## Plot and explore the eigenvalues\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_hbq6_OXCL4"
   },
   "source": [
    "   \n",
    "Finally, we will examine the eigenvalues of the covariance matrix. Remember that each eigenvalue describes the variance of the data projected onto its corresponding eigenvector. This is an important concept because it allows us to rank the PCA basis vectors based on how much variance each one can capture. First run the code below to plot the eigenvalues (sometimes called the \"scree plot\"). Which eigenvalue is larger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZF2anMJDXCL4"
   },
   "outputs": [],
   "source": [
    "plot_eigenvalues(evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMcw04CYXCL4"
   },
   "source": [
    "## Interactive Demo: Exploration of the correlation coefficient\n",
    "\n",
    "Run the following cell and use the slider to change the correlation coefficient in the data. You should see the scree plot and the plot of basis vectors update.\n",
    "\n",
    "**Questions:**\n",
    "* What happens to the eigenvalues as you change the correlation coefficient?\n",
    "* Can you find a value for which both eigenvalues are equal?\n",
    "* Can you find a value for which only one eigenvalue is nonzero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "m1rIUJ4RXCL4"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\n",
    "# @markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "\n",
    "def refresh(corr_coef=.8):\n",
    "  cov_matrix = calculate_cov_matrix(variance_1, variance_2, corr_coef)\n",
    "  X = get_data(cov_matrix)\n",
    "  score, evectors, evals = pca(X)\n",
    "  plot_eigenvalues(evals)\n",
    "  plot_basis_vectors(X, evectors)\n",
    "\n",
    "\n",
    "_ = widgets.interact(refresh, corr_coef=(-1, 1, .1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBkfJ1YwXCL4"
   },
   "source": [
    "---\n",
    "# Summary\n",
    "- In this tutorial, we learned that goal of PCA is to find an orthonormal basis capturing the directions of maximum variance of the data. More precisely, the $i$th basis vector is the direction that maximizes the projected variance, while being orthogonal to all previous basis vectors. Mathematically, these basis vectors are the eigenvectors of the covariance matrix (also called *loadings*). \n",
    "- PCA also has the useful property that the projected data (*scores*) are uncorrelated.\n",
    "- The projected variance along each basis vector is given by its corresponding eigenvalue. This is important because it allows us rank the \"importance\" of each basis vector in terms of how much of the data variability it explains. An eigenvalue of zero means there is no variation along that direction so it can be dropped without losing any information about the original data.\n",
    "- In the next tutorial, we will use this property to reduce the dimensionality of high-dimensional data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBIwjd7ZXCL4"
   },
   "source": [
    "---\n",
    "# Bonus: Mathematical basis of PCA properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "QO4nVJo8XCL4"
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Properties of PCA\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"p56UrMRt6-U\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsYQxao_XMX1"
   },
   "source": [
    "# Dimensionality Reduction and reconstruction\n",
    "\n",
    "__Content creators:__ Alex Cayco Gajic, John Murray\n",
    "\n",
    "__Content reviewers:__ Roozbeh Farhoudi, Matt Krause, Spiros Chavlis, Richard Gao, Michael Waskom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecVJ_jEVXMX7"
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "In this notebook we'll learn to apply PCA for dimensionality reduction, using a classic dataset that is often used to benchmark machine learning algorithms: MNIST. We'll also learn how to use PCA for reconstruction and denoising.\n",
    "\n",
    "Overview:\n",
    "- Perform PCA on MNIST\n",
    "- Calculate the variance explained\n",
    "- Reconstruct data with different numbers of PCs\n",
    "- (Bonus) Examine denoising using PCA\n",
    "\n",
    "You can learn more about MNIST dataset [here](https://en.wikipedia.org/wiki/MNIST_database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nOTXlcWfXMX8"
   },
   "outputs": [],
   "source": [
    "# @title Video 1: PCA for dimensionality reduction\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"oO0bbInoO_0\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXhHHGGMXMX8"
   },
   "source": [
    "---\n",
    "# Setup\n",
    "Run these cells to get the tutorial started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZqgCA1pXMX8"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "smVSaSlpXMX9"
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets   # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "t0H95LugXMX9"
   },
   "outputs": [],
   "source": [
    "# @title Helper Functions\n",
    "\n",
    "\n",
    "def plot_variance_explained(variance_explained):\n",
    "  \"\"\"\n",
    "  Plots eigenvalues.\n",
    "\n",
    "  Args:\n",
    "    variance_explained (numpy array of floats) : Vector of variance explained\n",
    "                                                 for each PC\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(np.arange(1, len(variance_explained) + 1), variance_explained,\n",
    "           '--k')\n",
    "  plt.xlabel('Number of components')\n",
    "  plt.ylabel('Variance explained')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_MNIST_reconstruction(X, X_reconstructed):\n",
    "  \"\"\"\n",
    "  Plots 9 images in the MNIST dataset side-by-side with the reconstructed\n",
    "  images.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats)               : Data matrix each column\n",
    "                                              corresponds to a different\n",
    "                                              random variable\n",
    "    X_reconstructed (numpy array of floats) : Data matrix each column\n",
    "                                              corresponds to a different\n",
    "                                              random variable\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  ax = plt.subplot(121)\n",
    "  k = 0\n",
    "  for k1 in range(3):\n",
    "    for k2 in range(3):\n",
    "      k = k + 1\n",
    "      plt.imshow(np.reshape(X[k, :], (28, 28)),\n",
    "                 extent=[(k1 + 1) * 28, k1 * 28, (k2 + 1) * 28, k2 * 28],\n",
    "                 vmin=0, vmax=255)\n",
    "  plt.xlim((3 * 28, 0))\n",
    "  plt.ylim((3 * 28, 0))\n",
    "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
    "                  labelbottom=False)\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  plt.title('Data')\n",
    "  plt.clim([0, 250])\n",
    "  ax = plt.subplot(122)\n",
    "  k = 0\n",
    "  for k1 in range(3):\n",
    "    for k2 in range(3):\n",
    "      k = k + 1\n",
    "      plt.imshow(np.reshape(np.real(X_reconstructed[k, :]), (28, 28)),\n",
    "                 extent=[(k1 + 1) * 28, k1 * 28, (k2 + 1) * 28, k2 * 28],\n",
    "                 vmin=0, vmax=255)\n",
    "  plt.xlim((3 * 28, 0))\n",
    "  plt.ylim((3 * 28, 0))\n",
    "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
    "                  labelbottom=False)\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  plt.clim([0, 250])\n",
    "  plt.title('Reconstructed')\n",
    "  plt.tight_layout()\n",
    "\n",
    "\n",
    "def plot_MNIST_sample(X):\n",
    "  \"\"\"\n",
    "  Plots 9 images in the MNIST dataset.\n",
    "\n",
    "  Args:\n",
    "     X (numpy array of floats) : Data matrix each column corresponds to a\n",
    "                                 different random variable\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  k = 0\n",
    "  for k1 in range(3):\n",
    "    for k2 in range(3):\n",
    "      k = k + 1\n",
    "      plt.imshow(np.reshape(X[k, :], (28, 28)),\n",
    "                 extent=[(k1 + 1) * 28, k1 * 28, (k2+1) * 28, k2 * 28],\n",
    "                 vmin=0, vmax=255)\n",
    "  plt.xlim((3 * 28, 0))\n",
    "  plt.ylim((3 * 28, 0))\n",
    "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
    "                  labelbottom=False)\n",
    "  plt.clim([0, 250])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_MNIST_weights(weights):\n",
    "  \"\"\"\n",
    "  Visualize PCA basis vector weights for MNIST. Red = positive weights,\n",
    "  blue = negative weights, white = zero weight.\n",
    "\n",
    "  Args:\n",
    "     weights (numpy array of floats) : PCA basis vector\n",
    "\n",
    "  Returns:\n",
    "     Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  cmap = plt.cm.get_cmap('seismic')\n",
    "  plt.imshow(np.real(np.reshape(weights, (28, 28))), cmap=cmap)\n",
    "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
    "                  labelbottom=False)\n",
    "  plt.clim(-.15, .15)\n",
    "  plt.colorbar(ticks=[-.15, -.1, -.05, 0, .05, .1, .15])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def add_noise(X, frac_noisy_pixels):\n",
    "  \"\"\"\n",
    "  Randomly corrupts a fraction of the pixels by setting them to random values.\n",
    "\n",
    "  Args:\n",
    "     X (numpy array of floats)  : Data matrix\n",
    "     frac_noisy_pixels (scalar) : Fraction of noisy pixels\n",
    "\n",
    "  Returns:\n",
    "     (numpy array of floats)    : Data matrix + noise\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  X_noisy = np.reshape(X, (X.shape[0] * X.shape[1]))\n",
    "  N_noise_ixs = int(X_noisy.shape[0] * frac_noisy_pixels)\n",
    "  noise_ixs = np.random.choice(X_noisy.shape[0], size=N_noise_ixs,\n",
    "                               replace=False)\n",
    "  X_noisy[noise_ixs] = np.random.uniform(0, 255, noise_ixs.shape)\n",
    "  X_noisy = np.reshape(X_noisy, (X.shape[0], X.shape[1]))\n",
    "\n",
    "  return X_noisy\n",
    "\n",
    "\n",
    "def change_of_basis(X, W):\n",
    "  \"\"\"\n",
    "  Projects data onto a new basis.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponding to a\n",
    "                                different random variable\n",
    "    W (numpy array of floats) : new orthonormal basis columns correspond to\n",
    "                                basis vectors\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : Data matrix expressed in new basis\n",
    "  \"\"\"\n",
    "\n",
    "  Y = np.matmul(X, W)\n",
    "\n",
    "  return Y\n",
    "\n",
    "\n",
    "def get_sample_cov_matrix(X):\n",
    "  \"\"\"\n",
    "  Returns the sample covariance matrix of data X.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
    "                                different random variable\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : Covariance matrix\n",
    "\"\"\"\n",
    "\n",
    "  X = X - np.mean(X, 0)\n",
    "  cov_matrix = 1 / X.shape[0] * np.matmul(X.T, X)\n",
    "  return cov_matrix\n",
    "\n",
    "\n",
    "def sort_evals_descending(evals, evectors):\n",
    "  \"\"\"\n",
    "  Sorts eigenvalues and eigenvectors in decreasing order. Also aligns first two\n",
    "  eigenvectors to be in first two quadrants (if 2D).\n",
    "\n",
    "  Args:\n",
    "    evals (numpy array of floats)    :   Vector of eigenvalues\n",
    "    evectors (numpy array of floats) :   Corresponding matrix of eigenvectors\n",
    "                                         each column corresponds to a different\n",
    "                                         eigenvalue\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)          : Vector of eigenvalues after sorting\n",
    "    (numpy array of floats)          : Matrix of eigenvectors after sorting\n",
    "  \"\"\"\n",
    "\n",
    "  index = np.flip(np.argsort(evals))\n",
    "  evals = evals[index]\n",
    "  evectors = evectors[:, index]\n",
    "  if evals.shape[0] == 2:\n",
    "    if np.arccos(np.matmul(evectors[:, 0],\n",
    "                           1 / np.sqrt(2) * np.array([1, 1]))) > np.pi / 2:\n",
    "      evectors[:, 0] = -evectors[:, 0]\n",
    "    if np.arccos(np.matmul(evectors[:, 1],\n",
    "                           1 / np.sqrt(2)*np.array([-1, 1]))) > np.pi / 2:\n",
    "      evectors[:, 1] = -evectors[:, 1]\n",
    "\n",
    "  return evals, evectors\n",
    "\n",
    "\n",
    "def pca(X):\n",
    "  \"\"\"\n",
    "  Performs PCA on multivariate data. Eigenvalues are sorted in decreasing order\n",
    "\n",
    "  Args:\n",
    "     X (numpy array of floats) :   Data matrix each column corresponds to a\n",
    "                                   different random variable\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)    : Data projected onto the new basis\n",
    "    (numpy array of floats)    : Vector of eigenvalues\n",
    "    (numpy array of floats)    : Corresponding matrix of eigenvectors\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  X = X - np.mean(X, 0)\n",
    "  cov_matrix = get_sample_cov_matrix(X)\n",
    "  evals, evectors = np.linalg.eigh(cov_matrix)\n",
    "  evals, evectors = sort_evals_descending(evals, evectors)\n",
    "  score = change_of_basis(X, evectors)\n",
    "\n",
    "  return score, evectors, evals\n",
    "\n",
    "\n",
    "def plot_eigenvalues(evals, limit=True):\n",
    "  \"\"\"\n",
    "  Plots eigenvalues.\n",
    "\n",
    "  Args:\n",
    "     (numpy array of floats) : Vector of eigenvalues\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(np.arange(1, len(evals) + 1), evals, 'o-k')\n",
    "  plt.xlabel('Component')\n",
    "  plt.ylabel('Eigenvalue')\n",
    "  plt.title('Scree plot')\n",
    "  if limit:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2Ntea3SXMYA"
   },
   "source": [
    "---\n",
    "# Section 1: Perform PCA on MNIST\n",
    "\n",
    "The MNIST dataset consists of a 70,000 images of individual handwritten digits. Each image is a 28x28 pixel grayscale image. For convenience, each 28x28 pixel image is often unravelled into a single 784 (=28*28) element vector, so that the whole dataset is represented as a 70,000 x 784 matrix. Each row represents a different image, and each column represents a different pixel.\n",
    " \n",
    "Enter the following cell to load the MNIST dataset and plot the first nine images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSiXt-14XMYC"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(name='mnist_784')\n",
    "X = mnist.data\n",
    "plot_MNIST_sample(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKDwXlqgXMYD"
   },
   "source": [
    "The MNIST dataset has an extrinsic dimensionality of 784, much higher than the 2-dimensional examples used in the previous tutorials! To make sense of this data, we'll use dimensionality reduction. But first, we need to determine the intrinsic dimensionality $K$ of the data. One way to do this is to look for an \"elbow\" in the scree plot, to determine which eigenvalues are signficant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvyd7sXZXMYD"
   },
   "source": [
    "## Exercise 1: Scree plot of MNIST\n",
    "\n",
    "In this exercise you will examine the scree plot in the MNIST dataset.\n",
    "\n",
    "**Steps:**\n",
    "- Perform PCA on the dataset and examine the scree plot. \n",
    "- When do the eigenvalues appear (by eye) to reach zero? (**Hint:** use `plt.xlim` to zoom into a section of the plot).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58T0Wt78XMYE"
   },
   "outputs": [],
   "source": [
    "help(pca)\n",
    "help(plot_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFFzh1DXXMYE"
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TO DO for students: perform PCA and plot the eigenvalues\n",
    "#################################################\n",
    "\n",
    "# perform PCA\n",
    "# score, evectors, evals = ...\n",
    "# plot the eigenvalues\n",
    "# plot_eigenvalues(evals, limit=False)\n",
    "# plt.xlim(...)  # limit x-axis up to 100 for zooming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFxo_QeVXMYE"
   },
   "source": [
    "---\n",
    "# Section 2: Calculate the variance explained\n",
    "\n",
    "The scree plot suggests that most of the eigenvalues are near zero, with fewer than 100 having large values. Another common way to determine the intrinsic dimensionality is by considering the variance explained. This can be examined with a cumulative plot of the fraction of the total variance explained by the top $K$ components, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{var explained} = \\frac{\\sum_{i=1}^K \\lambda_i}{\\sum_{i=1}^N \\lambda_i}\n",
    "\\end{equation}\n",
    "\n",
    "The intrinsic dimensionality is often quantified by the $K$ necessary to explain a large proportion of the total variance of the data (often a defined threshold, e.g., 90%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIBUhqJAXMYF"
   },
   "source": [
    "## Exercise 2: Plot the explained variance\n",
    "\n",
    "In this exercise you will plot the explained variance.\n",
    "\n",
    "**Steps:**\n",
    "- Fill in the function below to calculate the fraction variance explained as a function of the number of principal componenets. **Hint:** use `np.cumsum`.\n",
    "- Plot the variance explained using `plot_variance_explained`.\n",
    "\n",
    "**Questions:**\n",
    "- How many principal components are required to explain 90% of the variance?\n",
    "- How does the intrinsic dimensionality of this dataset compare to its extrinsic dimensionality?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hr5QMkDnXMYF"
   },
   "outputs": [],
   "source": [
    "help(plot_variance_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZTHLi9OXMYF"
   },
   "outputs": [],
   "source": [
    "def get_variance_explained(evals):\n",
    "  \"\"\"\n",
    "  Calculates variance explained from the eigenvalues.\n",
    "\n",
    "  Args:\n",
    "    evals (numpy array of floats) : Vector of eigenvalues\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)       : Vector of variance explained\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  ## TO DO for students: calculate the explained variance using the equation\n",
    "  ## from Section 2.\n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"Student excercise: calculate explaine variance!\")\n",
    "  #################################################\n",
    "\n",
    "  # cumulatively sum the eigenvalues\n",
    "  csum = ...\n",
    "  # normalize by the sum of eigenvalues\n",
    "  variance_explained = ...\n",
    "\n",
    "  return variance_explained\n",
    "\n",
    "\n",
    "#################################################\n",
    "## TO DO for students: call the function and plot the variance explained\n",
    "#################################################\n",
    "\n",
    "# calculate the variance explained\n",
    "variance_explained = ...\n",
    "\n",
    "# Uncomment to plot the variance explained\n",
    "# plot_variance_explained(variance_explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1gp9LjmXMYF"
   },
   "source": [
    "---\n",
    "# Section 3: Reconstruct data with different numbers of PCs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_n7GtGBWXMYG"
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Data Reconstruction\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"ZCUhW26AdBQ\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6XPOKHfXMYG"
   },
   "source": [
    "Now we have seen that the top 100 or so principal components of the data can explain most of the variance. We can use this fact to perform *dimensionality reduction*, i.e., by storing the data using only 100 components rather than the samples of all 784 pixels. Remarkably, we will be able to reconstruct much of the structure of the data using only the top 100 components. To see this, recall that to perform PCA we projected the data $\\bf X$ onto the eigenvectors of the covariance matrix:\n",
    "\\begin{equation}\n",
    "\\bf S = X W\n",
    "\\end{equation}\n",
    "Since $\\bf W$ is an orthogonal matrix, ${\\bf W}^{-1} = {\\bf W}^T$. So by multiplying by ${\\bf W}^T$ on each side we can rewrite this equation as  \n",
    "\\begin{equation}\n",
    "{\\bf X = S W}^T.\n",
    "\\end{equation}\n",
    "This now gives us a way to reconstruct the data matrix from the scores and loadings. To reconstruct the data from a low-dimensional approximation, we just have to truncate these matrices.  Let's call ${\\bf S}_{1:K}$ and ${\\bf W}_{1:K}$ as keeping only the first $K$ columns of this matrix. Then our reconstruction is:\n",
    "\\begin{equation}\n",
    "{\\bf \\hat X = S}_{1:K} ({\\bf W}_{1:K})^T.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZPVO_LgXMYG"
   },
   "source": [
    "## Exercise 3: Data reconstruction\n",
    "\n",
    "Fill in the function below to reconstruct the data using different numbers of principal components. \n",
    "\n",
    "**Steps:**\n",
    "\n",
    "* Fill in the following function to reconstruct the data based on the weights and scores. Don't forget to add the mean!\n",
    "* Make sure your function works by reconstructing the data with all $K=784$ components. The two images should look identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RazLR0-TXMYG"
   },
   "outputs": [],
   "source": [
    "help(plot_MNIST_reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rE8dBAD2XMYG"
   },
   "outputs": [],
   "source": [
    "def reconstruct_data(score, evectors, X_mean, K):\n",
    "  \"\"\"\n",
    "  Reconstruct the data based on the top K components.\n",
    "\n",
    "  Args:\n",
    "    score (numpy array of floats)    : Score matrix\n",
    "    evectors (numpy array of floats) : Matrix of eigenvectors\n",
    "    X_mean (numpy array of floats)   : Vector corresponding to data mean\n",
    "    K (scalar)                       : Number of components to include\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)          : Matrix of reconstructed data\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  ## TO DO for students: Reconstruct the original data in X_reconstructed\n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"Student excercise: reconstructing data function!\")\n",
    "  #################################################\n",
    "\n",
    "  # Reconstruct the data from the score and eigenvectors\n",
    "  # Don't forget to add the mean!!\n",
    "  X_reconstructed =  ...\n",
    "\n",
    "  return X_reconstructed\n",
    "\n",
    "\n",
    "K = 784\n",
    "\n",
    "#################################################\n",
    "## TO DO for students: Calculate the mean and call the function, then plot\n",
    "## the original and the recostructed data\n",
    "#################################################\n",
    "\n",
    "# Reconstruct the data based on all components\n",
    "X_mean = ...\n",
    "X_reconstructed = ...\n",
    "\n",
    "# Plot the data and reconstruction\n",
    "# plot_MNIST_reconstruction(X, X_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y05ZFoeOXMYH"
   },
   "source": [
    "## Interactive Demo: Reconstruct the data matrix using different numbers of PCs\n",
    "\n",
    "Now run the code below and experiment with the slider to reconstruct the data matrix using different numbers of principal components.\n",
    "\n",
    "**Steps**\n",
    "* How many principal components are necessary to reconstruct the numbers (by eye)? How does this relate to the intrinsic dimensionality of the data?\n",
    "* Do you see any information in the data with only a single principal component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "o2W56TQ2XMYH"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\n",
    "# @markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "\n",
    "def refresh(K=100):\n",
    "  X_reconstructed = reconstruct_data(score, evectors, X_mean, K)\n",
    "  plot_MNIST_reconstruction(X, X_reconstructed)\n",
    "  plt.title('Reconstructed, K={}'.format(K))\n",
    "\n",
    "\n",
    "_ = widgets.interact(refresh, K=(1, 784, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rjJLfkdXMYH"
   },
   "source": [
    "## Exercise 4: Visualization of the weights\n",
    "\n",
    "Next, let's take a closer look at the first principal component by visualizing its corresponding weights. \n",
    "\n",
    "**Steps:**\n",
    "\n",
    "* Enter `plot_MNIST_weights` to visualize the weights of the first basis vector.\n",
    "* What structure do you see? Which pixels have a strong positive weighting? Which have a strong negative weighting? What kinds of images would this basis vector differentiate?\n",
    "* Try visualizing the second and third basis vectors. Do you see any structure? What about the 100th basis vector? 500th? 700th?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tl0vbZmdXMYH"
   },
   "outputs": [],
   "source": [
    "help(plot_MNIST_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ef1wRtTXMYH"
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TO DO for students: plot the weights calling the plot_MNIST_weights function\n",
    "#################################################\n",
    "\n",
    "# Plot the weights of the first principal component\n",
    "# plot_MNIST_weights(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BT3e6EOEXMYI"
   },
   "source": [
    "---\n",
    "# Summary\n",
    "* In this tutorial, we learned how to use PCA for dimensionality reduction by selecting the top principal components. This can be useful as the intrinsic dimensionality ($K$) is often less than the extrinsic dimensionality ($N$) in neural data. $K$ can be inferred by choosing the number of eigenvalues necessary to capture some fraction of the variance.\n",
    "* We also learned how to reconstruct an approximation of the original data using the top $K$ principal components. In fact, an alternate formulation of PCA is to find the $K$ dimensional space that minimizes the reconstruction error.\n",
    "* Noise tends to inflate the apparent intrinsic dimensionality, however the higher components reflect noise rather than new structure in the data. PCA can be used for denoising data by removing noisy higher components.\n",
    "* In MNIST, the weights corresponding to the first principal component appear to discriminate between a 0 and 1. We will discuss the implications of this for data visualization in the following tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxMJ5-vzXMYI"
   },
   "source": [
    "---\n",
    "# Bonus: Examine denoising using PCA\n",
    "\n",
    "In this lecture, we saw that PCA finds an optimal low-dimensional basis to minimize the reconstruction error. Because of this property, PCA can be useful for denoising corrupted samples of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DQyNGx3XMYI"
   },
   "source": [
    "## Exercise 5: Add noise to the data\n",
    "In this exercise you will add salt-and-pepper noise to the original data and see how that affects the eigenvalues. \n",
    "\n",
    "**Steps:**\n",
    "- Use the function `add_noise` to add noise to 20% of the pixels.\n",
    "- Then, perform PCA and plot the variance explained. How many principal components are required to explain 90% of the variance? How does this compare to the original data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DydVIupjXMYI"
   },
   "outputs": [],
   "source": [
    "help(add_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0xTVdfGXMYI"
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Insert your code here to:\n",
    "# Add noise to the data\n",
    "# Plot noise-corrupted data\n",
    "# Perform PCA on the noisy data\n",
    "# Calculate and plot the variance explained\n",
    "###################################################################\n",
    "np.random.seed(2020)  # set random seed\n",
    "X_noisy = ...\n",
    "# score_noisy, evectors_noisy, evals_noisy = ...\n",
    "# variance_explained_noisy = ...\n",
    "# plot_MNIST_sample(X_noisy)\n",
    "# plot_variance_explained(variance_explained_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbVJ8LqxXMYJ"
   },
   "source": [
    "## Exercise 6: Denoising\n",
    "\n",
    "Next, use PCA to perform denoising by projecting the noise-corrupted data onto the basis vectors found from the original dataset. By taking the top K components of this projection, we can reduce noise in dimensions orthogonal to the K-dimensional latent space. \n",
    "\n",
    "**Steps:**\n",
    "- Subtract the mean of the noise-corrupted data.\n",
    "- Project the data onto the basis found with the original dataset (`evectors`, not `evectors_noisy`) and take the top $K$ components. \n",
    "- Reconstruct the data as normal, using the top 50 components. \n",
    "- Play around with the amount of noise and K to build intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zy1SAYuAXMYJ"
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Insert your code here to:\n",
    "# Subtract the mean of the noise-corrupted data\n",
    "# Project onto the original basis vectors evectors\n",
    "# Reconstruct the data using the top 50 components\n",
    "# Plot the result\n",
    "###################################################################\n",
    "\n",
    "X_noisy_mean = ...\n",
    "projX_noisy = ...\n",
    "X_reconstructed = ...\n",
    "# plot_MNIST_reconstruction(X_noisy, X_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dk7OokmXs7x"
   },
   "source": [
    "# Dimensionality Reduction: Nonlinear dimensionality reduction\n",
    "\n",
    "__Content creators:__ Alex Cayco Gajic, John Murray\n",
    "\n",
    "__Content reviewers:__ Roozbeh Farhoudi, Matt Krause, Spiros Chavlis, Richard Gao, Michael Waskom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7yDo-sMXs74"
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "In this notebook we'll explore how dimensionality reduction can be useful for visualizing and inferring structure in your data. To do this, we will compare PCA with t-SNE, a nonlinear dimensionality reduction method.\n",
    "\n",
    "Overview:\n",
    "- Visualize MNIST in 2D using PCA.\n",
    "- Visualize MNIST in 2D using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "6ePaTIdEXs75"
   },
   "outputs": [],
   "source": [
    "# @title Video 1: PCA Applications\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"2Zb93aOWioM\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snyVQh1sXs76"
   },
   "source": [
    "---\n",
    "# Setup\n",
    "Run these cells to get the tutorial started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y91I3ekdXs76"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7Eb5HsJ4Xs76"
   },
   "outputs": [],
   "source": [
    "#@title Figure Settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4r9ENS-aXs77"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "\n",
    "\n",
    "def visualize_components(component1, component2, labels, show=True):\n",
    "  \"\"\"\n",
    "  Plots a 2D representation of the data for visualization with categories\n",
    "  labelled as different colors.\n",
    "\n",
    "  Args:\n",
    "    component1 (numpy array of floats) : Vector of component 1 scores\n",
    "    component2 (numpy array of floats) : Vector of component 2 scores\n",
    "    labels (numpy array of floats)     : Vector corresponding to categories of\n",
    "                                         samples\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  cmap = plt.cm.get_cmap('tab10')\n",
    "  plt.scatter(x=component1, y=component2, c=labels, cmap=cmap)\n",
    "  plt.xlabel('Component 1')\n",
    "  plt.ylabel('Component 2')\n",
    "  plt.colorbar(ticks=range(10))\n",
    "  plt.clim(-0.5, 9.5)\n",
    "  if show:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCktuluvXs77"
   },
   "source": [
    "---\n",
    "# Section 1: Visualize MNIST in 2D using PCA\n",
    "\n",
    "In this exercise, we'll visualize the first few components of the MNIST dataset to look for evidence of structure in the data. But in this tutorial, we will also be interested in the label of each image (i.e., which numeral it is from 0 to 9). Start by running the following cell to reload the MNIST dataset (this takes a few seconds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPeePikIXs78"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(name='mnist_784')\n",
    "X = mnist.data\n",
    "labels = [int(k) for k in mnist.target]\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8cRE3eCXs78"
   },
   "source": [
    "To perform PCA, we now will use the method implemented in sklearn. Run the following cell to set the parameters of PCA - we will only look at the top 2 components because we will be visualizing the data in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfPPw8sRXs78"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_model = PCA(n_components=2) # Initializes PCA\n",
    "pca_model.fit(X) # Performs PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02tklVi3Xs79"
   },
   "source": [
    "## Exercise 1: Visualization of MNIST in 2D using PCA\n",
    "\n",
    "Fill in the code below to perform PCA and visualize the top two  components. For better visualization, take only the first 2,000 samples of the data (this will also make t-SNE much faster in the following section of the tutorial so don't skip this step!)\n",
    "\n",
    "**Suggestions:**\n",
    "- Truncate the data matrix at 2,000 samples. You will also need to truncate the array of labels.\n",
    "- Perform PCA on the truncated data.\n",
    "- Use the function `visualize_components` to plot the labelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TC1vokqXs79"
   },
   "outputs": [],
   "source": [
    "help(visualize_components)\n",
    "help(pca_model.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFR8L3FrXs79"
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TODO for students: take only 2,000 samples and perform PCA\n",
    "#################################################\n",
    "\n",
    "# Take only the first 2000 samples with the corresponding labels\n",
    "# X, labels = ...\n",
    "# Perform PCA\n",
    "# scores = pca_model.transform(X)\n",
    "\n",
    "# Plot the data and reconstruction\n",
    "# visualize_components(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2vSIJ0bXs7-"
   },
   "source": [
    "## Think!\n",
    "- What do you see? Are different samples corresponding to the same numeral clustered together? Is there much overlap?\n",
    "- Do some pairs of numerals appear to be more distinguishable than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1NN0bFFXs7-"
   },
   "source": [
    "---\n",
    "# Section 2: Visualize MNIST in 2D using t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "RQi9yoU5Xs7-"
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Nonlinear Methods\n",
    "video = YouTubeVideo(id=\"5Xpb0YaN5Ms\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ4UDu6lXs7-"
   },
   "source": [
    "Next we will analyze the same data using t-SNE, a nonlinear dimensionality reduction method that is useful for visualizing high dimensional data in 2D or 3D. Run the cell below to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AG8-9B_aXs7_"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, perplexity=30, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuA2KXAhXs7_"
   },
   "source": [
    "## Exercise 2: Apply t-SNE on MNIST\n",
    "First, we'll run t-SNE on the data to explore whether we can see more structure. The cell above defined the parameters that we will use to find our embedding (i.e, the low-dimensional representation of the data) and stored them in `model`. To run t-SNE on our data, use the function `model.fit_transform`.\n",
    "\n",
    "**Suggestions:**\n",
    "- Run t-SNE using the function `model.fit_transform`.\n",
    "- Plot the result data using `visualize_components`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPqUCu86Xs7_"
   },
   "outputs": [],
   "source": [
    "help(tsne_model.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lAddevDXs8A"
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TODO for students: perform tSNE and visualize the data\n",
    "#################################################\n",
    "\n",
    "# perform t-SNE\n",
    "embed = ...\n",
    "\n",
    "# Visualize the data\n",
    "# visualize_components(..., ..., labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tjNebvnXs8B"
   },
   "source": [
    "## Exercise 3: Run t-SNE with different perplexities\n",
    "\n",
    "Unlike PCA, t-SNE has a free parameter (the perplexity) that roughly determines how global vs. local information is weighted. Here we'll take a look at how the perplexity affects our interpretation of the results. \n",
    "\n",
    "**Steps:**\n",
    "- Rerun t-SNE (don't forget to re-initialize using the function `TSNE` as above) with a perplexity of 50, 5 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8_qh1v-Xs8B"
   },
   "outputs": [],
   "source": [
    "def explore_perplexity(values):\n",
    "  \"\"\"\n",
    "  Plots a 2D representation of the data for visualization with categories\n",
    "  labelled as different colors using different perplexities.\n",
    "\n",
    "  Args:\n",
    "    values (list of floats) : list with perplexities to be visualized\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "  for perp in values:\n",
    "\n",
    "    #################################################\n",
    "    ## TO DO for students: Insert your code here to redefine the t-SNE \"model\"\n",
    "    ## while setting the perplexity perform t-SNE on the data and plot the\n",
    "    ## results for perplexity = 50, 5, and 2 (set random_state to 2020\n",
    "    # Comment these lines when you complete the function\n",
    "    raise NotImplementedError(\"Student Exercise! Explore t-SNE with different perplexity\")\n",
    "    #################################################\n",
    "\n",
    "    # perform t-SNE\n",
    "    tsne_model = ...\n",
    "\n",
    "    embed = tsne_model.fit_transform(X)\n",
    "    visualize_components(embed[:, 0], embed[:, 1], labels, show=False)\n",
    "    plt.title(f\"perplexity: {perp}\")\n",
    "\n",
    "\n",
    "# Uncomment when you complete the function\n",
    "# values = [50, 5, 2]\n",
    "# explore_perplexity(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuqhQDzFXs8C"
   },
   "source": [
    "## Think!\n",
    "\n",
    "- What changes compared to your previous results using perplexity equal to 50? Do you see any clusters that have a different structure than before? \n",
    "- What changes in the embedding structure for perplexity equals to 5 or 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8hvA0oMXs8C"
   },
   "source": [
    "---\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR0Rb6mAXs8D"
   },
   "source": [
    "* We learned the difference between linear and nonlinear dimensionality reduction. While nonlinear methods can be more powerful, they can also be senseitive to noise. In contrast, linear methods are useful for their simplicity and robustness.\n",
    "* We compared PCA and t-SNE for data visualization. Using t-SNE, we could visualize clusters in the data corresponding to different digits. While PCA was able to separate some clusters (e.g., 0 vs 1), it performed poorly overall.\n",
    "* However, the results of t-SNE can change depending on the choice of perplexity. To learn more, we recommend this [Distill paper](https://distill.pub/2016/misread-tsne/).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "W1D5_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
