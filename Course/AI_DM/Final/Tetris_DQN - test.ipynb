{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym\n",
    "import gym_tetris\n",
    "from gym_tetris.actions import MOVEMENT\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "# from tensorflow.keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "# 사용할 수 있는 GPU를 gpus에 저장\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# 나는 2번 GPU만 사용하도록 설정했다. \n",
    "# EX) 0번 GPU가 사용하고 싶다면 gpus[2]를 gpus[0]으로 바꿔주면 된다.\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, action_size, state_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu',\n",
    "                            input_shape=state_size)\n",
    "        self.conv2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')\n",
    "        self.conv3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.fc = Dense(512, activation='relu')\n",
    "        self.fc_out = Dense(action_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        q = self.fc_out(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, action_size, state_size=(84, 84, 4)):\n",
    "        self.render = False\n",
    "\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # DQN 하이퍼파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 1e-4\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end  = 0.1\n",
    "        self.exploration_steps = 1000000\n",
    "        self.epsilon_decay_step = self.epsilon_start - self.epsilon_end\n",
    "        self.epsilon_decay_step /= self.exploration_steps\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "\n",
    "        # 리플레이 메모리, 최대 크기 100,000\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        \n",
    "#         # 게임 시작 후 랜덤하게 움직이지 않는 것에 대한 옵션\n",
    "#         self.no_op_steps = 30\n",
    "\n",
    "        # 모델과 타깃 모델 생성\n",
    "        self.model = DQN(action_size, state_size)\n",
    "        self.target_model = DQN(action_size, state_size)\n",
    "        self.optimizer = Adam(self.learning_rate, clipnorm=10.)\n",
    "        \n",
    "        # 타깃 모델 초기화\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "\n",
    "        self.writer = tf.summary.create_file_writer('summary/Tetris_DQN')\n",
    "        self.model_path = os.path.join(os.getcwd(), 'save_model', 'model')\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, history):\n",
    "        \n",
    "        # action selection with e-greedy policy\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model(history)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def append_sample(self, history, action, reward, next_history):\n",
    "        # 샘플 <s, a, r, s'>을 메모리에 저장\n",
    "        self.memory.append((history, action, reward, next_history, done))\n",
    "\n",
    "    # 텐서보드에 학습 정보를 기록\n",
    "    def draw_tensorboard(self, score, step, episode):\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('Total Reward/Episode', score, step=episode)\n",
    "            tf.summary.scalar('Average Max Q/Episode',\n",
    "                              self.avg_q_max / float(step), step=episode)\n",
    "            tf.summary.scalar('Duration/Episode', step, step=episode)\n",
    "            tf.summary.scalar('Average Loss/Episode',\n",
    "                              self.avg_loss / float(step), step=episode)\n",
    "\n",
    "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "#         temp_batch = np.array(batch)\n",
    "#         print(temp_batch.shape)\n",
    "#         print(\"\\n\")\n",
    "\n",
    "        history = np.array([sample[0][0] / 255. for sample in batch],\n",
    "                           dtype=np.float32)\n",
    "        actions = np.array([sample[1] for sample in batch])\n",
    "        rewards = np.array([sample[2] for sample in batch])\n",
    "        next_history = np.array([sample[3][0] / 255. for sample in batch],\n",
    "                                dtype=np.float32)\n",
    "#         dones = np.array([sample[4] for sample in batch])\n",
    "#         print(dones)\n",
    "#         print(\"==============================================================================\")\n",
    "\n",
    "        # 학습 파라메터\n",
    "        model_params = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 현재 상태에 대한 모델의 큐함수\n",
    "            predicts = self.model(history)\n",
    "            one_hot_action = tf.one_hot(actions, self.action_size)\n",
    "            predicts = tf.reduce_sum(one_hot_action * predicts, axis=1)\n",
    "\n",
    "            # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "            target_predicts = self.target_model(next_history)\n",
    "\n",
    "            # 벨만 최적 방정식을 구성하기 위한 타깃과 큐함수의 최대 값 계산\n",
    "            max_q = np.amax(target_predicts, axis=1)\n",
    "            targets = rewards + self.discount_factor * max_q\n",
    "\n",
    "            # 후버로스 계산\n",
    "            error = tf.abs(targets - predicts)\n",
    "            quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "            linear_part = error - quadratic_part\n",
    "            loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "\n",
    "            self.avg_loss += loss.numpy()\n",
    "\n",
    "        # 오류함수를 줄이는 방향으로 모델 업데이트\n",
    "        grads = tape.gradient(loss, model_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "\n",
    "\n",
    "def pre_processing(observe):\n",
    "    # RGB to GRAY\n",
    "    processed_observe = np.uint8(resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    \n",
    "    return processed_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:     0 | score:  2.0 | score max :  2.0 | score avg:  2.0 | memory length:  7890 | epsilon: 1.000 | q avg : 0.05 | avg loss : 0.00\n",
      "episode:     1 | score:  2.0 | score max :  2.0 | score avg:  2.0 | memory length: 16494 | epsilon: 1.000 | q avg : 0.05 | avg loss : 0.00\n",
      "episode:     2 | score:  0.0 | score max :  2.0 | score avg:  1.8 | memory length: 26203 | epsilon: 1.000 | q avg : 0.05 | avg loss : 0.00\n",
      "episode:     3 | score:  1.0 | score max :  2.0 | score avg:  1.7 | memory length: 36834 | epsilon: 1.000 | q avg : 0.05 | avg loss : 0.00\n",
      "episode:     4 | score:  0.0 | score max :  2.0 | score avg:  1.5 | memory length: 43316 | epsilon: 1.000 | q avg : 0.05 | avg loss : 0.00\n",
      "episode:     5 | score:  0.0 | score max :  2.0 | score avg:  1.4 | memory length: 52414 | epsilon: 0.998 | q avg : 0.06 | avg loss : 0.00\n",
      "episode:     6 | score:  0.0 | score max :  2.0 | score avg:  1.3 | memory length: 58389 | epsilon: 0.992 | q avg : 0.08 | avg loss : 0.00\n",
      "episode:     7 | score: 40.0 | score max : 40.0 | score avg:  5.1 | memory length: 68102 | epsilon: 0.984 | q avg : 0.08 | avg loss : 0.00\n",
      "episode:     8 | score:  0.0 | score max : 40.0 | score avg:  4.6 | memory length: 76229 | epsilon: 0.976 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:     9 | score:  0.0 | score max : 40.0 | score avg:  4.2 | memory length: 86817 | epsilon: 0.967 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    10 | score:  0.0 | score max : 40.0 | score avg:  3.7 | memory length: 97203 | epsilon: 0.958 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    11 | score:  2.0 | score max : 40.0 | score avg:  3.6 | memory length: 100000 | epsilon: 0.950 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    12 | score:  1.0 | score max : 40.0 | score avg:  3.3 | memory length: 100000 | epsilon: 0.941 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    13 | score:  0.0 | score max : 40.0 | score avg:  3.0 | memory length: 100000 | epsilon: 0.934 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    14 | score:  0.0 | score max : 40.0 | score avg:  2.7 | memory length: 100000 | epsilon: 0.926 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    15 | score:  1.0 | score max : 40.0 | score avg:  2.5 | memory length: 100000 | epsilon: 0.918 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    16 | score:  0.0 | score max : 40.0 | score avg:  2.3 | memory length: 100000 | epsilon: 0.910 | q avg : 0.08 | avg loss : 0.00\n",
      "episode:    17 | score:  4.0 | score max : 40.0 | score avg:  2.4 | memory length: 100000 | epsilon: 0.905 | q avg : 0.08 | avg loss : 0.00\n",
      "episode:    18 | score:  0.0 | score max : 40.0 | score avg:  2.2 | memory length: 100000 | epsilon: 0.897 | q avg : 0.08 | avg loss : 0.00\n",
      "episode:    19 | score:  1.0 | score max : 40.0 | score avg:  2.1 | memory length: 100000 | epsilon: 0.888 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    20 | score:  2.0 | score max : 40.0 | score avg:  2.1 | memory length: 100000 | epsilon: 0.882 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    21 | score:  0.0 | score max : 40.0 | score avg:  1.9 | memory length: 100000 | epsilon: 0.872 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    22 | score:  1.0 | score max : 40.0 | score avg:  1.8 | memory length: 100000 | epsilon: 0.866 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    23 | score:  2.0 | score max : 40.0 | score avg:  1.8 | memory length: 100000 | epsilon: 0.858 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    24 | score:  0.0 | score max : 40.0 | score avg:  1.6 | memory length: 100000 | epsilon: 0.851 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    25 | score:  2.0 | score max : 40.0 | score avg:  1.7 | memory length: 100000 | epsilon: 0.842 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    26 | score:  1.0 | score max : 40.0 | score avg:  1.6 | memory length: 100000 | epsilon: 0.833 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    27 | score:  0.0 | score max : 40.0 | score avg:  1.4 | memory length: 100000 | epsilon: 0.824 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    28 | score:  1.0 | score max : 40.0 | score avg:  1.4 | memory length: 100000 | epsilon: 0.817 | q avg : 0.09 | avg loss : 0.00\n",
      "episode:    29 | score:  0.0 | score max : 40.0 | score avg:  1.2 | memory length: 100000 | epsilon: 0.810 | q avg : 0.10 | avg loss : 0.00\n",
      "episode:    30 | score:  1.0 | score max : 40.0 | score avg:  1.2 | memory length: 100000 | epsilon: 0.805 | q avg : 0.10 | avg loss : 0.00\n",
      "episode:    31 | score:  2.0 | score max : 40.0 | score avg:  1.3 | memory length: 100000 | epsilon: 0.798 | q avg : 0.10 | avg loss : 0.00\n",
      "episode:    32 | score:  2.0 | score max : 40.0 | score avg:  1.4 | memory length: 100000 | epsilon: 0.793 | q avg : 0.10 | avg loss : 0.00\n",
      "episode:    33 | score:  2.0 | score max : 40.0 | score avg:  1.4 | memory length: 100000 | epsilon: 0.787 | q avg : 0.10 | avg loss : 0.00\n",
      "episode:    34 | score:  1.0 | score max : 40.0 | score avg:  1.4 | memory length: 100000 | epsilon: 0.779 | q avg : 0.10 | avg loss : 0.00\n",
      "episode:    35 | score:  0.0 | score max : 40.0 | score avg:  1.3 | memory length: 100000 | epsilon: 0.771 | q avg : 0.10 | avg loss : 0.00\n",
      "episode:    36 | score:  2.0 | score max : 40.0 | score avg:  1.3 | memory length: 100000 | epsilon: 0.766 | q avg : 0.10 | avg loss : 0.00\n",
      "episode:    37 | score: 44.0 | score max : 44.0 | score avg:  5.6 | memory length: 100000 | epsilon: 0.757 | q avg : 0.11 | avg loss : 0.00\n",
      "episode:    38 | score:  2.0 | score max : 44.0 | score avg:  5.2 | memory length: 100000 | epsilon: 0.752 | q avg : 0.11 | avg loss : 0.00\n",
      "episode:    39 | score:  3.0 | score max : 44.0 | score avg:  5.0 | memory length: 100000 | epsilon: 0.747 | q avg : 0.11 | avg loss : 0.00\n",
      "episode:    40 | score:  2.0 | score max : 44.0 | score avg:  4.7 | memory length: 100000 | epsilon: 0.739 | q avg : 0.11 | avg loss : 0.00\n",
      "episode:    41 | score:  3.0 | score max : 44.0 | score avg:  4.5 | memory length: 100000 | epsilon: 0.733 | q avg : 0.11 | avg loss : 0.00\n",
      "episode:    42 | score:  2.0 | score max : 44.0 | score avg:  4.3 | memory length: 100000 | epsilon: 0.727 | q avg : 0.11 | avg loss : 0.00\n",
      "episode:    43 | score:  1.0 | score max : 44.0 | score avg:  4.0 | memory length: 100000 | epsilon: 0.721 | q avg : 0.11 | avg loss : 0.00\n",
      "episode:    44 | score:  4.0 | score max : 44.0 | score avg:  4.0 | memory length: 100000 | epsilon: 0.717 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    45 | score:  5.0 | score max : 44.0 | score avg:  4.1 | memory length: 100000 | epsilon: 0.711 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    46 | score:  3.0 | score max : 44.0 | score avg:  4.0 | memory length: 100000 | epsilon: 0.705 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    47 | score:  8.0 | score max : 44.0 | score avg:  4.4 | memory length: 100000 | epsilon: 0.698 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    48 | score:  4.0 | score max : 44.0 | score avg:  4.3 | memory length: 100000 | epsilon: 0.693 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    49 | score:  0.0 | score max : 44.0 | score avg:  3.9 | memory length: 100000 | epsilon: 0.690 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    50 | score:  7.0 | score max : 44.0 | score avg:  4.2 | memory length: 100000 | epsilon: 0.687 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    51 | score:  2.0 | score max : 44.0 | score avg:  4.0 | memory length: 100000 | epsilon: 0.682 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    52 | score:  6.0 | score max : 44.0 | score avg:  4.2 | memory length: 100000 | epsilon: 0.677 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    53 | score:  6.0 | score max : 44.0 | score avg:  4.4 | memory length: 100000 | epsilon: 0.672 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    54 | score:  4.0 | score max : 44.0 | score avg:  4.3 | memory length: 100000 | epsilon: 0.665 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    55 | score:  6.0 | score max : 44.0 | score avg:  4.5 | memory length: 100000 | epsilon: 0.661 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    56 | score:  7.0 | score max : 44.0 | score avg:  4.7 | memory length: 100000 | epsilon: 0.657 | q avg : 0.12 | avg loss : 0.00\n",
      "episode:    57 | score:  7.0 | score max : 44.0 | score avg:  5.0 | memory length: 100000 | epsilon: 0.652 | q avg : 0.13 | avg loss : 0.00\n",
      "episode:    58 | score:  4.0 | score max : 44.0 | score avg:  4.9 | memory length: 100000 | epsilon: 0.649 | q avg : 0.13 | avg loss : 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:    59 | score:  6.0 | score max : 44.0 | score avg:  5.0 | memory length: 100000 | epsilon: 0.645 | q avg : 0.13 | avg loss : 0.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 환경과 DQN 에이전트 생성\n",
    "    env = gym_tetris.make('TetrisA-v0')\n",
    "    env = JoypadSpace(env, MOVEMENT)\n",
    "    agent = Agent(action_size=12)\n",
    "\n",
    "    global_step = 0\n",
    "    score_avg = 0\n",
    "    score_max = 0\n",
    "\n",
    "    num_episode = 50000\n",
    "    \n",
    "    for e in range(num_episode):\n",
    "        done = False\n",
    "\n",
    "        step, score = 0, 0, \n",
    "        # env 초기화\n",
    "        observe = env.reset()\n",
    "\n",
    "#         # 랜덤으로 뽑힌 값 만큼의 프레임동안 움직이지 않음\n",
    "#         for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "#             observe, _, _, _ = env.step(1)\n",
    "\n",
    "        # 프레임을 전처리 한 후 4개의 상태를 쌓아서 입력값으로 사용.\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            # 바로 전 history를 입력으로 받아 행동을 선택\n",
    "            action = agent.get_action(history)\n",
    "\n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            observe, reward, done, info = env.step(action)\n",
    "            # 각 타임스텝마다 상태 전처리\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "            agent.avg_q_max += np.amax(agent.model(np.float32(history / 255.0))[0])\n",
    "\n",
    "            score += reward\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장 후 학습\n",
    "            agent.append_sample(history, action, reward, next_history)\n",
    "\n",
    "            # 리플레이 메모리 크기가 정해놓은 수치에 도달한 시점부터 모델 학습 시작\n",
    "            if len(agent.memory) >= agent.train_start:\n",
    "                agent.train_model()\n",
    "                # 일정 시간마다 타겟모델을 모델의 가중치로 업데이트\n",
    "                if global_step % agent.update_target_rate == 0:\n",
    "                    agent.update_target_model()\n",
    "\n",
    "            if done:\n",
    "                # 각 에피소드 당 학습 정보를 기록\n",
    "                if global_step > agent.train_start:\n",
    "                    agent.draw_tensorboard(int(score), step, e)\n",
    "\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                score_max = score if score > score_max else score_max\n",
    "\n",
    "                log = \"episode: {:5d} | \".format(e)\n",
    "                log += \"score: {:4.1f} | \".format(score)\n",
    "                log += \"score max : {:4.1f} | \".format(score_max)\n",
    "                log += \"score avg: {:4.1f} | \".format(score_avg)\n",
    "                log += \"memory length: {:5d} | \".format(len(agent.memory))\n",
    "                log += \"epsilon: {:.3f} | \".format(agent.epsilon)\n",
    "                log += \"q avg : {:3.2f} | \".format(agent.avg_q_max / float(step))\n",
    "                log += \"avg loss : {:3.2f}\".format(agent.avg_loss / float(step))\n",
    "                print(log)\n",
    "\n",
    "                agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "\n",
    "        # 1000 에피소드마다 모델 저장\n",
    "        if e % 1000 == 0:\n",
    "            agent.model.save_weights(\"./save_model/model\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym_tetris.make('TetrisA-v0')\n",
    "# env = JoypadSpace(env, MOVEMENT)\n",
    "# state, reward, done, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
