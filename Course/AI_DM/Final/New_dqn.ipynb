{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import gym_tetris\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_tetris.actions import MOVEMENT\n",
    "\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, action_size, state_size):\n",
    "        self.action_size = action_size \n",
    "        self.state_size = state_size  # State dimensions, 4\n",
    "        self.epsilon = 1.0  # Random action rate\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.model = self.__init_model()  # Q-value function based on neural network\n",
    "        self.target_model = self.__init_model()  # Additional target model for more stable learning\n",
    "        self.update_target_model()  # Copy weights of main model onto target\n",
    "\n",
    "        self.min_epsilon = 0.01  # Lower bound of exploration\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.gamma = 0.95  # Reward discount rate\n",
    "\n",
    "        # For storing past experiences (samples)\n",
    "        # Oldest sample is discarded when length exceeds 'maxlen'\n",
    "        self.memory = deque(maxlen=5120)\n",
    "        self.training_threshold = 2560  # Sample threshold for starting to use memory\n",
    "        self.batch_size = 32  # How many samples to train on at a time\n",
    "\n",
    "\n",
    "    def __init_model(self):\n",
    "        # Neural network for approximating Q(s,a)\n",
    "        # Input: array of state values\n",
    "        # Output: estimated values for all actions (in this case two)\n",
    "        init = tf.keras.initializers.RandomUniform(-1e-3, 1e-3)\n",
    "        \n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(Input(shape=[84,84,4]))\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size, kernel_initializer=init))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Get epsilon greedy action\n",
    "        state = np.float32(state / 255.0)\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "\n",
    "    def memorize(self, sample):\n",
    "        self.memory.append(sample)\n",
    "\n",
    "\n",
    "    def _split_batch(self, training_batch):\n",
    "        # Separates a training batch into arrays of states, actions, etc.\n",
    "\n",
    "        states = np.array([sample[0][0] / 255. for sample in training_batch])\n",
    "        actions = np.array([sample[1] for sample in training_batch])\n",
    "        rewards = np.array([sample[2] for sample in training_batch])\n",
    "        next_states = np.array([sample[3][0] / 255. for sample in training_batch])\n",
    "        dones = np.array([sample[4] for sample in training_batch])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "\n",
    "    def training_step(self):\n",
    "        # Performs one training step of our model\n",
    "\n",
    "        training_batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = self._split_batch(training_batch)\n",
    "        \n",
    "        # Array of predicted q values for the next state\n",
    "        target_qs = self.target_model.predict(next_states)\n",
    "\n",
    "        # Our temporal difference targets\n",
    "        targets = rewards + self.gamma * np.amax(target_qs, axis=-1) * (1 - dones)\n",
    "\n",
    "        # Array of predicted q values for current state\n",
    "        current_qs = self.model.predict(states)\n",
    "\n",
    "        self.model.train_on_batch(x=states, y=current_qs) #y의 shape이 32*12\n",
    "\n",
    "        # Decaying exploration\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "def pre_processing(observe):\n",
    "    # RGB to GRAY\n",
    "    processed_observe = np.uint8(resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    \n",
    "    return processed_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1/500, score: 1, avg score: 1.00, max_score: 1, epsilon: 0.087, steps done: 5000, mem length: 5000\n",
      "ep 2/500, score: 0, avg score: 0.90, max_score: 1, epsilon: 0.01, steps done: 4743, mem length: 5120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4c96658e62af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# we don't train until we have enough samples in our memory to randomize from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_threshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-b212848fb291>\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Array of predicted q values for current state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mcurrent_qs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurrent_qs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#y의 shape이 32*12\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dl_gan\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dl_gan\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m     return self._model_iteration(\n\u001b[0;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dl_gan\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m           model, mode)\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m       callbacks = cbks.configure_callbacks(\n",
      "\u001b[1;32m~\\.conda\\envs\\dl_gan\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m     if (context.executing_eagerly()\n\u001b[0;32m    331\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[1;32m--> 332\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIteratorV2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\.conda\\envs\\dl_gan\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    591\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dl_gan\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    597\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[0mds_variant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dl_gan\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mautotune\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m       \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ModelDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpu_budget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_stats\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_stats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregator\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dl_gan\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, algorithm, cpu_budget)\u001b[0m\n\u001b[0;32m   3720\u001b[0m           \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3721\u001b[0m           \u001b[0mcpu_budget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcpu_budget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3722\u001b[1;33m           **self._flat_structure)\n\u001b[0m\u001b[0;32m   3723\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ModelDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dl_gan\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmodel_dataset\u001b[1;34m(input_dataset, output_types, output_shapes, algorithm, cpu_budget, name)\u001b[0m\n\u001b[0;32m   3226\u001b[0m         \u001b[1;34m\"ModelDataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3227\u001b[0m         \u001b[1;34m\"algorithm\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cpu_budget\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpu_budget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3228\u001b[1;33m         output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   3229\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3230\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Runs the training of the model\n",
    "RENDER = False\n",
    "env = gym_tetris.make('TetrisA-v0')\n",
    "env = JoypadSpace(env, MOVEMENT)\n",
    "\n",
    "# Joypad MOVEMENT: 12\n",
    "action_size = len(MOVEMENT)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "agent = QAgent(action_size, state_size)\n",
    "n_episodes = 501\n",
    "average_scores = []\n",
    "avg_score = 0\n",
    "max_score = 0\n",
    "\n",
    "for e in range(1, n_episodes):\n",
    "    state = env.reset()\n",
    "    state = pre_processing(state)\n",
    "    history = np.stack((state, state, state, state), axis=2)\n",
    "    history = np.reshape([history], (1, 84, 84, 4))\n",
    "        \n",
    "    done = False\n",
    "    time_step = 0\n",
    "    score = 0  # Score for evaluation of agent\n",
    "\n",
    "    while not done and time_step < 5000:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        action = agent.get_action(history)\n",
    "        next_state, reward, done, info = env.step(action)  # Environment interaction\n",
    "\n",
    "        score += reward\n",
    "        reward = 0.1 if not done else -1  # Negative reward if the game has ended\n",
    "\n",
    "        next_state = pre_processing(next_state)\n",
    "        next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "        next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            \n",
    "        sample = (history, action, reward, next_history, done)\n",
    "\n",
    "        agent.memorize(sample)  # Store the sample in memory\n",
    "\n",
    "        # In order to avoid the problem with correlating samples,\n",
    "        # we don't train until we have enough samples in our memory to randomize from\n",
    "        if len(agent.memory) >= agent.training_threshold:\n",
    "            agent.training_step()\n",
    "\n",
    "        state = next_state\n",
    "        time_step += 1\n",
    "\n",
    "    agent.update_target_model()\n",
    "    avg_score = 0.9 * avg_score + 0.1 * score if avg_score != 0 else score\n",
    "    max_score = score if score > max_score else max_score\n",
    "    average_scores.append(avg_score)\n",
    "\n",
    "    print('ep {}/{}, score: {}, avg score: {:3.2f}, max_score: {}, epsilon: {:.2}, steps done: {}, mem length: {}'\n",
    "          .format(e, n_episodes-1, score, avg_score, max_score, agent.epsilon, time_step, len(agent.memory)))\n",
    "        \n",
    "    if e % 100 == 0:\n",
    "        agent.model.save_weights(\"./saved_models/model\", save_format=\"tf\")\n",
    "        \n",
    "# Plot of average scores        \n",
    "plt.plot(average_scores, 'r')\n",
    "plt.plot(average_scores, 'b')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
