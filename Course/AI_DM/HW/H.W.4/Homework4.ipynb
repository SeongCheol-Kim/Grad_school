{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seongcheol Kim  \n",
    "2020178469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement adaptive epsilon greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = [ 1.624, -0.612, -0.528, -1.073, 0.865, -2.302, 1.745, -0.761, 0.319, -0.249]\n",
    "epsilon = 0.1\n",
    "num_episode = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_epsilon_greedy(mu, epsilon, num_episodes):\n",
    "\n",
    "    # write a boolean expression that determines if we should take the best action\n",
    "    be_greedy = np.random.random() > epsilon\n",
    "    if be_greedy:\n",
    "        # write an expression for selecting the best action from the action values\n",
    "        action = np.argmax(mu)\n",
    "    else:\n",
    "        # write an expression for selecting a random action\n",
    "        action = np.random.choice(len(mu))\n",
    "    \n",
    "    #decaying epsilon\n",
    "    if epsilon > 0:\n",
    "        epsilon -= epsilon/num_episodes\n",
    "\n",
    "    return action, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "action, epsilon = adaptive_epsilon_greedy(mu, epsilon, num_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0999"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement Softmax action selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_action_selection(mu):\n",
    "\n",
    "    #prob of each arm\n",
    "    probs_n = np.exp(mu)\n",
    "    \n",
    "    #sum of all probs\n",
    "    probs_d = probs_n.sum()\n",
    "    \n",
    "    probs = probs_n / probs_d\n",
    "    \n",
    "\n",
    "    cum_prob = 0.\n",
    "    z = np.random.uniform(0,1)\n",
    "    for idx, prob in enumerate(probs):  \n",
    "        cum_prob += prob\n",
    "        if cum_prob > z:\n",
    "            return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = softmax_action_selection(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combine epsilon-greedy action policy and softmax action policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_action_selection(mu):\n",
    "    \n",
    "    choice = random.randint(1, 10)\n",
    "    \n",
    "    if choice % 10 != 0:\n",
    "        probs_n = np.exp(mu)\n",
    "        probs_d = probs_n.sum()\n",
    "        probs = probs_n / probs_d\n",
    "\n",
    "        cum_prob = 0.\n",
    "        z = np.random.uniform(0,1)\n",
    "        for idx, prob in enumerate(probs):  \n",
    "            cum_prob += prob\n",
    "            if cum_prob > z:\n",
    "                return idx\n",
    "    \n",
    "    else:\n",
    "        # write an expression for selecting a random action\n",
    "        action = np.random.choice(len(mu))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = combined_action_selection(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compare three algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E-greedy often randomly selects in the beginning, and decreases random selections toward the later stages of learning.  \n",
    "Softmax selection is a method of using all estimates for each action. The action itself is randomly selected, but the action with a higher estimate is selected with a higher probability.  \n",
    "The combined selection performs softmax selection with 90% probability and random selection with 10% probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
